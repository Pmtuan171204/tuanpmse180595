[
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders Purpose of the Event The event was organized to provide an in-depth perspective on the journey of digital transformation on the cloud computing platform in Vietnam. The main objectives include: AWS presenting its vision, strategy, and trends related to Cloud \u0026amp; GenAI for the Vietnamese market. Large enterprises such as Techcombank and U2U Network sharing real-world experiences in modernizing their systems. Workshops and discussion sessions offering practical knowledge for builders on Migration, Modernization, Security, and application development with GenAI. List of Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Jun Kai Loke – AI/ML Specialist SA, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Highlighted Content Opening Keynote: AWS introduced strategic directions in Vietnam and new opportunities for the builder generation. Real-world sharing: Techcombank and U2U Network presented lessons learned during their digital transformation journey. Panel “GenAI Revolution”: Leaders of many organizations discussed how to integrate AI into their development strategies. Migration \u0026amp; Modernization Track: Case study on migrating large workloads to AWS. Demonstration of Amazon Q Developer helping automate the software development process. Discussion about modernizing applications and security in the era of rapidly developing AI. Deep technical sessions: Roadmap for migrating VMware systems to AWS (EKS, RDS, serverless). Large-scale security models with analytical support from GenAI. What I Learned Every technical decision should begin with business needs. Effective migration should be divided into phases rather than shifting everything at once. Amazon Q Developer can significantly shorten the software development lifecycle. Security must be built into the design from the beginning, not added later. Application to Work Conduct event-storming with the team to identify suitable GenAI use cases. Try integrating Amazon Q Developer into the current development process. Apply a phased migration approach to upcoming cloud projects. Combine DevSecOps to ensure security throughout development and operation. Experience at the Event This was my first time attending Vietnam Cloud Day, and it truly left many impressions:\nListening directly to CEOs and CTOs helped me understand the bigger picture of digital transformation in Vietnam. The GenAI discussion panel was very practical — each speaker had different viewpoints, but all emphasized the strong connection between AI and business strategy. The Amazon Q Developer demo helped me clearly visualize the power of GenAI across the entire SDLC. I had the opportunity to talk with many experts and builders, gaining many useful insights. Key Takeaways GenAI is powerful, but success still depends on people and strategy. Cloud transformation must have a clear roadmap — rushing is risky. The combination of Cloud and AI will accelerate the growth of Vietnamese businesses. Builders need to continuously learn and be willing to experiment with new technologies. Some photos from the event "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AI-Driven Development Life Cycle: Reimagining Software Engineering Event Objectives The event aimed to clarify the evolving role of AI in modern software development, specifically:\nIntroducing emerging trends in AI-Driven Development – an AI-supported and AI-orchestrated software development model. Presenting the AI-Driven Development Lifecycle (AI-DLC) framework, which integrates AI throughout the development process Demonstrating two representative tools: Amazon Q Developer and Kiro IDE Extension. Analyzing how AI enhances speed, productivity, and product quality for software development teams. Defining the future role of AI in development workflows. Speakers Lecturers: Toan Huynh \u0026amp; My Nguyen Facilitators: Diem My, Dai Truong, Dinh Nguyen Key Highlights Opening – The Future of Software Development with AI Mr. Toan Huynh presented the topic “Shaping the Future of Development,” describing the transition from traditional workflows to AI-Orchestrated Development, where AI coordinates various phases of product development — from planning and design to coding and operations.\nLimitations of Current Development Models Current approaches such as AI-Assisted Development and AI-Managed Development still face limitations in stability, reliability, and explainability. Thus, AI-Driven Development (AI-DD) is introduced as a balanced approach: AI provides deep support, but humans remain the central decision-makers.\nAI-Driven Development Lifecycle (AI-DLC) Framework AI-DLC consolidates three stages of AI maturity in the software development process:\nAI-Assisted Development: AI suggests code, helps fix bugs, checks syntax. AI-Driven Development: AI contributes to architecture design, planning, and technical proposals. AI-Managed Development: AI orchestrates the entire workflow, while humans provide final approval. In this model, AI acts as an “intelligent coordinator,” while developers validate and finalize results.\nBenefits of AI in Software Development The “AI in Development – Outcomes” section highlights seven core values:\nPredictability – improved stability and timeline forecasting. Velocity – faster time-to-market. Quality – fewer bugs and higher system reliability. Innovation – more creative solution paths. Developer Engagement – increased motivation and focus. Customer Satisfaction – better user experience and service quality. Productivity – optimized effort and time savings. SDLC Phases and AI’s Role According to the SDLC time distribution diagram, the process includes: Explore \u0026amp; Plan → Create → Test \u0026amp; Secure → Review \u0026amp; Deploy → Maintain, Transform \u0026amp; Modernize\nAI significantly reduces time spent in heavy phases such as testing, deployment, and maintenance through automation and smart analytics.\nStandard AI-DLC Workflow The standard AI-DLC model consists of four steps:\nRequirement – The Product Owner analyzes and collects requirements. Design – The system architect defines architecture, APIs, and workflows. Implementation – Software engineers code, test, and integrate. Deployment – System deployment and monitoring. AI accompanies all four steps to ensure clarity and alignment across roles.\nKey Features of the AI-DLC Workflow As described in “Key Workflow Features,” the process includes:\nRole Separation – clear division of responsibilities between business, architecture, and development. AI-Enhanced – AI generates role-specific contexts to provide optimal assistance. Iterative – continuous feedback loops across all phases. Template-Driven – standardized outputs using unified AIDLC templates. AI in Each Development Phase AI supports each step in three primary ways:\nSpecific Context – AI works with tailored personas (PM, Dev, Architect). Clear Inputs/Outputs – well-defined inputs and deliverables for each stage. Interactive Workflow – enhanced two-way interaction between AI and humans. Documentation – AI continuously updates all related documentation. Practical Demos: Amazon Q Developer \u0026amp; Kiro IDE Amazon Q Developer\nIntegrated directly into VS Code and Cloud9. Generates code, writes tests, creates documentation, proposes AWS architecture. Auto-updates prompt.md, suggests user stories, and supports CI/CD. The demo showcased AI-driven project planning and management via commands. Kiro IDE (presented by My Nguyen)\nA tool that generates specification documents (requirements.md, design.md, tasks.md). AI helps define APIs, generate features, and produce backend code. The demo demonstrated AI automatically generating a chat application with user authentication. Key Takeaways AI is a collaborator, not a replacement for developers. AI-DLC makes the development lifecycle clearer, more standardized, and easier to control. Amazon Q Developer significantly improves speed and reduces errors in SDLC. Kiro IDE demonstrates AI’s capability to support everything from requirements to code generation. DevSecOps combined with AI will become an essential industry standard. Real-World Applications Using Amazon Q Developer to automate testing and documentation in internal projects. Applying Kiro IDE to standardize specifications and accelerate backend development. Running an experimental “AI-Driven Sprint” to measure the impact of AI within the team. Implementing AI-Assisted Code Review to improve product quality. Personal Reflection The workshop “AI-Driven Development Workshop” provided many practical insights.\nMr. Toan Huynh’s presentation helped me better understand the strategic role AI will play in the future of software development, while Ms. My Nguyen’s demo with Kiro IDE demonstrated how AI can fully support the process from requirement analysis to code generation.\nThe event changed my perspective on AI: it is not merely a supporting tool, but a foundation that enables organizations to innovate faster and more sustainably.\nSome event photos "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4-3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Workshop \u0026ldquo;Data science on AWS” - Unlock the Power of Data with Cloud Computing Event Objectives Exploring the journey of building a modern Data Science system.\nSpeakers Van Hoang Kha – Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong – Cloud DevOps Engineer, AWS Community Builder Key Highlights Overview of AI and Foundational Concepts (by Mr. Kha) Mr. Kha began the session by systematizing the essential concepts in the field of artificial intelligence:\nArtificial Intelligence (AI): Described as a “virtual assistant,” AI allows machines to simulate human-like learning and analytical abilities. AI can process large volumes of data at high speed and continuously improve by learning from new data. Machine Learning (ML): A subset of AI where systems learn patterns from numerical or text data to make predictions or classifications. Deep Learning (DL): A deep neural network approach that enables machines to understand complex data types such as images, audio, and unstructured data. Generative AI: A type of AI model trained on massive datasets to generate new content such as text, images, or videos. Examples include Google’s Gemini and AWS’s Amazon Bedrock. This represents a major advancement over ML and DL because the model can “create,” rather than just predict. AI/ML Service Layers on AWS (by Mr. Kha) Mr. Kha explained how AWS builds its AI ecosystem across three layers:\nLayer 1 – AI Services\nReady-to-use services where users call APIs to solve specific problems such as computer vision, natural language processing, or text-to-speech.\nLayer 2 – ML Services\nDesigned for users who need full control over data processing, model building, training, and fine-tuning. Amazon SageMaker is the central service, similar to Google Colab but more powerful and enterprise-ready.\nLayer 3 – ML Frameworks \u0026amp; Infrastructure\nAWS provides compute infrastructure with GPU/CPU hardware in collaboration with companies like NVIDIA to support training very large models.\nTypical AI Services on AWS Several powerful AI services were showcased:\nAmazon Comprehend: Natural language processing and text information extraction Amazon Translate: Deep learning–based translation Amazon Transcribe: Speech-to-text conversion Amazon Polly: Text-to-speech with natural-sounding voices Amazon Textract: Automated extraction from scanned documents and handwriting Amazon Rekognition: Face/object recognition and video analysis Amazon Personalize: Personalized recommendations like those used by Netflix or YouTube Machine Learning Model Building Workflow Mr. Kha shared the standard ML development lifecycle:\nFeature Engineering – Clean, transform, and prepare data Model Training – Teach the machine to learn patterns Evaluation \u0026amp; Tuning – Test accuracy and iterate if needed Deployment – Serve the model for real-world use Monitoring \u0026amp; Improvement – Collect feedback and update the model AWS also introduced SageMaker Canvas, a no-code tool for building and deploying ML models with a simple drag-and-drop interface.\nHands-on Demo (by Mr. Vương) Cleaning IMDb Data with AWS Glue\nRaw data stored in S3 AWS Glue automatically cleaned, processed, and normalized the data Processed data returned to S3, ready for training in SageMaker Cost comparison: Cloud vs. On-Premise\nCloud reduces upfront infrastructure and maintenance costs AWS allows scaling resources as needed, without hardware limits Experimenting with different models becomes faster and more flexible than with self-hosted systems Key Takeaways Strategy comes before technology: Digital transformation and GenAI adoption are most effective when driven by business needs—not trends. Migration requires a clear roadmap: Moving systems to the cloud should be done in phases to control risks and measure progress. GenAI accelerates software development: Through the Amazon Q Developer demo, I better understood how AI can automate many steps in the SDLC. Security must be built from the beginning: Security is not a “patching” stage—it must be embedded throughout the entire system design. People remain the decisive factor: No matter how advanced the technology is, success depends on mindset, learning ability, and execution. Practical Applications Conducting event storming sessions to identify AI use cases aligned with real business needs Integrating Amazon Q Developer into the current development workflow to measure improvements in coding, testing, and reviewing Planning phased cloud migration, starting with priority workloads to reduce risks Applying DevSecOps, ensuring end-to-end security from development to deployment Increasing internal discussions about AI/Cloud strategies for better team alignment Personal Reflections This event left a strong impression on me:\nThe GenAI discussion was highly practical, with varied viewpoints but a shared belief that AI must serve business goals. Watching the live demo of Amazon Q Developer gave me a clear vision of the future of AI-assisted software development. Networking with many experts and builders broadened my perspectives and provided valuable insights. Overall, the event was both inspirational and deeply practical, supporting my journey in Cloud and GenAI.\nSome event photos "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4-4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AI/ML/GenAI on AWS Workshop Event Objectives The workshop was designed to provide hands-on experience with AWS AI/ML services, focusing on Amazon SageMaker for traditional machine learning workflows and Amazon Bedrock for generative AI applications. The event aimed to help participants understand how to practically implement AI/ML solutions on AWS and explore the latest capabilities in generative AI.\nSpeakers Pham Nguyen Hai Anh - Cloud Engineer Program Overview 8:30 – 9:00 | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaking activities to encourage collaboration Overview of the AI/ML ecosystem in Vietnam 9:00 – 10:30 | Overview of AI/ML Services on AWS Amazon SageMaker – A Comprehensive ML Platform\nData Preparation \u0026amp; Labeling: Including data cleaning, feature engineering, and automated labeling capabilities. Model Training, Fine-Tuning \u0026amp; Deployment: Exploring SageMaker training infrastructure, hyperparameter tuning, and deployment options (real-time \u0026amp; batch inference). Integrated MLOps Capabilities: Built-in model versioning, monitoring, and automated retraining pipelines. Live Demo: SageMaker Studio Walkthrough\nThe demonstration introduced the unified development environment for machine learning, including:\nIntegrated Jupyter notebook Experiment tracking and model registry Visual workflow builder for MLOps pipelines Integration with other AWS data processing services 10:30 – 10:45 | Break Networking with refreshments and informal discussions on AI/ML use cases.\n10:45 – 12:00 | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – Comparison \u0026amp; Selection Guide\nUnderstanding available foundation models on Bedrock Comparing capabilities, use cases, and performance characteristics Best practices for choosing the right model based on business requirements Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nCore Prompt Engineering Principles: Learn how to create effective prompts to obtain the desired outputs from language models Chain-of-Thought Reasoning: Understand how to guide the model through step-by-step reasoning processes to solve complex problems Few-shot Learning: A technique that provides examples to improve the model’s performance on specific tasks without requiring fine-tuning Retrieval-Augmented Generation (RAG)\nRAG architecture overview: Understand how RAG combines relevant information retrieval with generative capabilities. Knowledge Base Integration: Learn how to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Deployment Patterns: Best practices for building RAG applications that deliver accurate, context-aware responses Bedrock Agents: Multi-Step Workflows and Tool Integration\nAgent Architecture: Understand how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learn how to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications capable of handling complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understand Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learn how to configure custom content filters based on business requirements Compliance and Governance: Best practices to ensure AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot with Bedrock\nDemo section guiding the creation of a complete chatbot application:\nSet up the Bedrock foundation model Implement RAG with integrated knowledge base Configure Bedrock Agents for multi-turn conversations Add Guardrails for content safety Deploy the chatbot application Key Takeaways Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring. Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case. RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance. Production-Ready MLOps: SageMaker’s integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production. Safety as a Priority: Bedrock Guardrails ensure generative AI applications are safe, compliant, and aligned with business values. What I Learned SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity. Choosing the right foundation model is essential and depends on the specific use case, performance requirements, and cost constraints. Prompt engineering is a critical skill that can greatly improve model output without requiring fine-tuning. RAG architecture is necessary for building AI applications that need access to specific, up-to-date information. Bedrock Agents enable building sophisticated AI applications capable of handling complex multi-step workflows. Content safety must be considered from the very beginning when building generative AI applications. Real-World Applications Experimenting with SageMaker: Set up SageMaker Studio to explore ML model development for data analytics projects. Building RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation systems and Q\u0026amp;A. Practicing Prompt Engineering: Develop prompt engineering skills by creating templates and best practices for common use cases. Integrating MLOps: Apply SageMaker’s MLOps capabilities to automate training and model deployment pipelines. Deploying Safely: Integrate Bedrock Guardrails into any generative AI application to ensure content safety. Personal Impressions This workshop provides an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can optimize the entire ML workflow. Learning about RAG architecture was eye-opening, demonstrating how to build AI applications that leverage specific knowledge bases. The Bedrock Agents demonstration highlighted the potential for building sophisticated AI applications capable of complex workflows. The practical focus on prompt engineering provided immediately applicable skills for working with language models. Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications. Key Takeaways Start with the use case: Always begin by defining the specific business problem before choosing an AI/ML solution. Powerful foundation models: Pre-trained foundation models can solve many problems without custom training. RAG is essential: For applications requiring domain-specific knowledge, RAG architecture is the right approach. MLOps matters: Proper MLOps practices are crucial for maintaining ML models in production. Safety is non-negotiable: Content filtering and safety measures must be integrated from the start. Continuous learning: The AI/ML ecosystem evolves rapidly, requiring ongoing learning and experimentation. Some event photos "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4-5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "DevOps on AWS Workshop Event Objectives The workshop is designed to provide comprehensive knowledge and hands-on experience with AWS DevOps services, including CI/CD pipelines, Infrastructure as Code, container services, and monitoring \u0026amp; observability. The event aims to help participants understand DevOps culture, principles, and best practices, while also exploring how to practically implement DevOps processes on AWS.\nSpeakers Pham Nguyen Hai Anh - Cloud Engineer Danh Hoang Hieu Nghi - GenAI Engineer Program Overview Morning Session (8:30 – 12:00) 8:30 – 9:00 | Welcome \u0026amp; DevOps Mindset\nRecap of the AI/ML session from the previous workshop DevOps Culture and Principles: Understand the cultural shift from traditional IT to DevOps, emphasizing collaboration, automation, and continuous improvement Benefits and Key Metrics: DORA Metrics: Deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate MTTR (Mean Time To Recovery): Measures how quickly teams can recover from failures Deployment Frequency: Tracks how often teams deploy code to production Discussion on how DevOps practices improve software delivery and operational performance 9:00 – 10:30 | AWS DevOps Services – CI/CD Pipeline Source Control: AWS CodeCommit, Git Strategies\nAWS CodeCommit: A fully managed, secure Git-based source control service Git Strategies: GitFlow: Workflow using feature branches, develop, and release branches Trunk-based Development: Focus on the main branch with short-lived feature branches Best practices for choosing a branching strategy based on team size and project requirements Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines\nAWS CodeBuild: A fully managed build service that compiles source code, runs tests, and produces deployable artifacts Build Configuration: Buildspec files, environment variables, and build artifacts Testing Pipelines: Unit tests, integration tests, and automated test execution Integration with testing frameworks and code quality tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates\nAWS CodeDeploy: Automated application deployments to EC2, Lambda, or on-premises servers Blue/Green Deployment: Zero-downtime deployment using two identical production environments Canary Deployment: Gradual rollout to a small percentage of users before full deployment Rolling Updates: Incremental deployment across instances with automatic rollback capability Selecting the right deployment strategy based on application requirements Orchestration: Automating with CodePipeline\nAWS CodePipeline: A fully managed continuous delivery service for automating release pipelines Pipeline Stages: Source, Build, Test, Deploy, and Approval Integrations: Connect CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automation: Automated triggers, parallel actions, and pipeline visualization Demo: Full CI/CD Pipeline Walkthrough The demo showcases a complete CI/CD pipeline:\nSet up a CodeCommit repository Configure CodeBuild for automated builds and tests Create a CodeDeploy application with Blue/Green deployment Build a CodePipeline to orchestrate the entire workflow Test the pipeline with code changes and observe automated deployment 10:30 – 10:45 | Break\nNetworking and refreshments.\n10:45 – 12:00 | Infrastructure as Code (IaC) AWS CloudFormation: Templates, Stacks và Drift Detection\nCloudFormation Templates: JSON/YAML templates defining AWS resources Stacks: Collections of AWS resources managed as a single unit Drift Detection: Detect changes made outside CloudFormation Stack Updates: Update infrastructure using change sets with rollback capability Best Practices: Template organization, parameterization, and nested stacks AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support\nAWS CDK: Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#, Go) Constructs: Reusable cloud components, from low-level resources to high-level patterns Reusable Patterns: Pre-built solutions for common use cases (VPC, ECS clusters, serverless applications) Language Support: TypeScript, Python, Java, C#, Go, and JavaScript Benefits: Type safety, IDE support, and easier testing compared to CloudFormation templates Demo: Deployment with CloudFormation and CDK\nThe demo highlights both approaches side-by-side:\nCloudFormation: Deploy a VPC and EC2 instance using a YAML template CDK: Deploy the same infrastructure using TypeScript and CDK constructs Demonstrates the differences in approach, maintainability, and overall developer experience Discussion: Choosing Between IaC Tools\nWhen to use CloudFormation vs CDK Considerations: team expertise, project complexity, and long-term maintainability Hybrid approach: Combining both tools for different parts of the infrastructure Lunch Break (12:00 – 13:00)\nAfternoon Session (13:00 – 17:00) 13:00 – 14:30 | Container Services on AWS Docker Fundamentals: Microservices \u0026amp; Containerization\nContainerization Concepts: Understanding containers, images, and the benefits of containerization Microservices Architecture: Breaking monolithic applications into smaller, independent services Docker Basics: Dockerfile, image building, and container lifecycle Benefits: Portability, consistency, and resource efficiency Amazon ECR: Image Storage, Scanning \u0026amp; Lifecycle Policies\nAmazon ECR: Fully managed Docker container registry Image Storage: Secure, scalable storage for Docker images Image Scanning: Automated vulnerability scanning Lifecycle Policies: Automated cleanup and image retention Integration: Seamless integration with ECS, EKS, and other AWS services Amazon ECS \u0026amp; EKS: Deployment Strategies, Scaling \u0026amp; Orchestration\nAmazon ECS: Fully managed container orchestration Task Definitions: Containers, resource requirements, and networking Services: Long-running tasks with load balancing and auto-scaling Deployment Strategies: Rolling updates, Blue/Green deployments Scaling: CPU/memory-based scaling or custom CloudWatch metrics Amazon EKS: Fully managed Kubernetes service Kubernetes Concepts: Pods, Services, Deployments, Namespaces EKS Features: Managed control plane, node groups, add-ons Deployment Strategies: Rolling updates, canary deployments via Istio/App Mesh Scaling: Cluster Autoscaler \u0026amp; Horizontal Pod Autoscaler AWS App Runner: Simplified Container Deployment\nApp Runner: Fully managed service for building and running containerized applications Easy Deployment: Deploy from source code or container image Auto-scaling: Fully automated, traffic-based scaling Use Cases: Web apps, APIs, microservices Comparison: When to use App Runner vs ECS vs EKS Demo \u0026amp; Case Study: Comparing Microservices Deployment Options The demo compares multiple container deployment methods:\nDeploy a simple web application using App Runner Deploy the same app using ECS Fargate Evaluate setup complexity, cost, and operational overhead Case study: Choosing the right container service for different business scenarios 14:30 – 14:45 | Break\n14:45 – 16:00 | Monitoring \u0026amp; Observability CloudWatch: Metrics, Logs, Alarms \u0026amp; Dashboards\nCloudWatch Metrics: Collect and monitor metrics from AWS services and custom apps CloudWatch Logs: Centralized logging with log groups \u0026amp; streams CloudWatch Alarms: Automated actions based on metric thresholds CloudWatch Dashboards: Customizable dashboards for visualization Best Practices: Metric naming, log retention, alarm design AWS X-Ray: Distributed Tracing và Performance Insights\nAWS X-Ray: Analyze and debug distributed applications Distributed Tracing: End-to-end request tracing across microservices Service Map: Visual representation of application architecture \u0026amp; dependencies Performance Insights: Identify bottlenecks and performance issues Integration: Integrate X-Ray SDK with applications and AWS services Demo: Full-Stack Observability Setup The demo includes:\nSetting up CloudWatch metrics and logs Creating dashboards for monitoring Configuring CloudWatch alarms for alerting Enabling X-Ray tracing Viewing service maps and trace analysis Best Practices: Alerting, Dashboards \u0026amp; On-Call Processes\nAlerting Strategy: Meaningful alerts, avoiding alert fatigue Dashboard Design: Tailored dashboards for developers, ops teams and leadership On-Call Processes: Incident response procedures, escalation paths, runbooks SLO/SLI: Measuring reliability effectively 16:00 – 16:45 | DevOps Best Practices \u0026amp; Case Studies Deployment Strategies: Feature Flags \u0026amp; A/B Testing\nFeature Flags: Gradual rollouts, canary releases, instant rollback A/B Testing: Compare versions to optimize user experience Tools: AWS AppConfig, LaunchDarkly integration Best Practices: Feature flag lifecycle management Automated Testing \u0026amp; CI/CD Integration\nTesting Pyramid: Unit, integration, and end-to-end testing Test Automation: Automated test execution in CI/CD pipelines Quality Gates: Blocking deployment based on test results Test Coverage: Measure and improve coverage Incident Management \u0026amp; Postmortems\nIncident Response: Detection, response, and recovery procedures Postmortems: Documenting root causes and learnings Blameless Culture: Focus on systems, not individuals Tools: Incident management tools and communication channels Case Studies: DevOps Transformation in Startups \u0026amp; Enterprises\nStartup Case Study: Fast scaling with DevOps practices, cost optimization Enterprise Case Study: Large-scale migration to DevOps and cultural transformation Key Lessons: Common challenges and solutions ROI: Measuring the impact of DevOps adoption 16:45 – 17:00 | Q\u0026amp;A \u0026amp; Closing\nDevOps Career Path: Growth roadmap and required skills AWS Certification Path: Relevant certs for DevOps engineers AWS Certified DevOps Engineer – Professional AWS Certified Solutions Architect AWS Certified SysOps Administrator Next Steps: Resources for continued learning Closing Summary: Key takeaways \u0026amp; action items Key Highlights CI/CD Pipeline: CodePipeline automates software delivery end-to-end Infrastructure as Code: CloudFormation and CDK both powerful; CDK offers superior developer experience Container Services: ECS, EKS, App Runner — flexibility for different use cases Observability: CloudWatch + X-Ray provide complete monitoring \u0026amp; tracing DevOps Culture: Tools are secondary—collaboration and mindset matter most Best Practices: Feature flags, automated testing, incident management What I Learned DevOps Is Culture First: Tools matter, but cultural transformation is the foundation CI/CD Automation: Significantly improves delivery speed and reliability Benefits of IaC: Version control, repeatability, and faster infrastructure changes Container Strategy: Choose the right service based on complexity and team expertise Observability is Essential: Monitoring \u0026amp; tracing are critical for reliability Continuous Improvement: DevOps is ongoing—not a one-time implementation Applying It to Work Implement CI/CD: Use CodePipeline for automated deployments Adopt IaC:: Start using CloudFormation or CDK Container Migration: Evaluate containerization opportunities Improve Monitoring: Enhance CloudWatch dashboards and alarms Practice DevOps Daily: Apply principles consistently Incident Management: Establish response procedures and postmortems Personal Experience This full-day DevOps workshop was comprehensive and highly practical:\nThe CI/CD pipeline demo was extremely valuable, showing complete automation IaC comparison clarified when to use CloudFormation vs CDK Container service comparison gave clear guidance for different scenarios Observability session emphasized the importance of monitoring \u0026amp; tracing Case studies provided real-world insights into DevOps transformation Career path discussion was motivating and gave clear direction Key Takeaways Start Small: Begin with basic CI/CD automation Culture Matters: Collaboration and mindset drive DevOps success Choose the Right Tools: Match tools with team skills and requirements Monitor Everything: Observability is essential Continuous Learning: DevOps evolves quickly Measure Success: Use DORA metrics and ROI analysis Một số hình ảnh khi tham gia sự kiện "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/4-6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "AWS Well-Architected Security Pillar Workshop Event Objectives The morning workshop provided a comprehensive and in-depth overview of the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nThe session was designed to equip participants with practical knowledge on implementing security best practices in AWS environments, supported by real-world examples from Vietnamese enterprises.\nSpeakers Đinh La Hoang Anh - GenAI Engineer Danh Hoang Hieu Nghi - GenAI Engineer Program Overview 8:30 – 8:50 AM | Opening \u0026amp; Security Foundations Security Pillar in the Well-Architected Framework\nThe role of the Security Pillar within the Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth AWS Shared Responsibility Model Top cloud security threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture\nIAM fundamentals: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO, permission sets SCPs \u0026amp; Permission Boundaries for multi-account environments MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring\nCloudTrail (organization-level), GuardDuty, Security Hub Logging across all layers: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 – 10:10 AM | Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security\nVPC segmentation, private vs. public placement Security Groups vs. NACLs: usage patterns WAF + Shield + Network Firewall Workload protection basics: EC2, ECS/EKS Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets\nKMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit for S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation\nIR lifecycle based on the AWS framework Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response with Lambda/Step Functions 11:40 – 12:00 PM | Summary \u0026amp; Q\u0026amp;A\nSummary of all five security pillars Common pitfalls \u0026amp; real-world cases from Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro certifications) Key Takeaways Least Privilege is fundamental—always start with minimum permissions and expand only when needed Zero Trust architecture assumes no implicit trust, even within internal networks Defense in Depth requires layered security controls Detection capabilities must be automated and continuous Incident response playbooks should be documented and tested regularly Practical Applications This workshop was extremely valuable for gaining a complete understanding of AWS security:\nThe hands-on demos made abstract concepts clear and practical Learning about common security pitfalls in Vietnamese businesses was highly useful The incident response playbooks offer templates that can be applied immediately Understanding how all five pillars connect helps in designing a holistic security architecture Lessons Learned Security is a continuous journey, not a final destination Automation is key to maintaining security at scale The Well-Architected Security Pillar provides a comprehensive cloud security framework Regular security reviews and improvements are essential AWS provides powerful native tools for each security domain Some photos from the event "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Pham Minh Tuan\nPhone Number: 0336108492\nEmail: phamminhtuan171204@gmail.com\nUniversity: FPT University HCMC\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerate AI development with Amazon Bedrock API keys by Sofian Hamiti, Ajit Mahareddy, Massimiliano Angelino, Huong Nguyen, and Nakul Vankadari Ramesh on 08 JUL 2025 in Amazon Bedrock, Amazon Machine Learning, Announcements, Artificial Intelligence, Foundation models Permalink Comments Share\nToday, we’re excited to announce a significant improvement to the developer experience of Amazon Bedrock: API keys. API keys provide a new way to access the Amazon Bedrock APIs, streamlining the authentication process so that developers can focus on building rather than configuration.\nCamelAI is an open-source, modular framework for building intelligent multi-agent systems for data generation, world simulation, and task automation.\n“As a startup with limited resources, streamlined customer onboarding is critical to our success. The Amazon Bedrock API keys enable us to onboard enterprise customers in minutes rather than hours. With Bedrock, our customers can quickly provision access to leading AI models and seamlessly integrate them into CamelAI,”\nsaid Miguel Salinas, CTO, CamelAI. In this post, explore how API keys work and how you can start using them today.\nAPI key authentication Amazon Bedrock now provides API key access to streamline integration with tools and frameworks that expect API key-based authentication. The Amazon Bedrock and Amazon Bedrock runtime SDKs support API key authentication for methods including on-demand inference, provisioned throughput inference, model fine-tuning, distillation, and evaluation.\nThe diagram compares the default authentication process to Amazon Bedrock (in orange) with the API keys approach (in blue). In the default process, you must create an identity in AWS IAM Identity Center or AWS IAM, attach IAM policies to provide permissions to perform API operations, and generate credentials, which you can then use to make API calls. The grey boxes in the diagram highlight the steps that Amazon Bedrock now streamlines when generating an API key. Developers can now authenticate and access Amazon Bedrock APIs with minimal setup overhead.\nYou can generate API keys in the Amazon Bedrock console, choosing between two types.\nShort-term API keys use the IAM permissions from your current IAM principal and expire when your account’s session ends or can last up to 12 hours, whichever ends first. Short-term API keys use AWS Signature Version 4 for authentication. For continuous application use, you can implement API key refreshing following those examples and using your credential provider of choice.\nWhen you create a long-term API key, Amazon Bedrock automatically creates an IAM user and associates the key with it. You can set expiration times ranging from 1 day to no expiration. Amazon Bedrock attaches the AmazonBedrockLimitedAccess managed policy to the IAM user, and you can modify permissions as needed through the IAM service. These keys are specific to Amazon Bedrock and cannot be used with other AWS services. We recommend using temporary AWS IAM credentials or short-term API keys for setups that require a higher level of security, and long-term keys with expiration dates for exploring Amazon Bedrock.\nMaking Your First API Call Once you have access to foundation models, getting started with Amazon Bedrock API key is straightforward. Here’s how to make your first API call using the AWS SDK for Python (Boto3 SDK) and API keys: Generate an API key To generate an API key, follow these steps:\n1.Sign in to the AWS Management Console and open the Amazon Bedrock console 2.In the left navigation panel, select API keys 3.Choose either Generate short-term API key or Generate long-term API key 4.For long-term keys, set your desired expiration time and optionally configure advanced permissions 5.Choose Generate and copy your API key Set Your API Key as Environment Variable\nYou can set your API key as an environment variable so that it’s automatically recognized when you make API requests:\n# To set the API key as an environment variable, you can open a terminal and run the following command: export AWS_BEARER_TOKEN_BEDROCK=\u0026lt;YOUR API KEY HERE\u0026gt; --- The Boto3 and AWS JavaScript SDKs automatically detect your environment variable when you create an Amazon Bedrock client. Make sure you use the latest SDK version.\nMake Your First API Call You can now make API calls to Amazon Bedrock in multiple ways: 1.Using curl\ncurl -X POST \u0026#34;https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $AWS_BEARER_TOKEN_BEDROCK\u0026#34; \\ -d \u0026#39;{ \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}] } ] }\u0026#39; 2.Using the Boto3 SDK for Amazon Bedrock:\nimport boto3 # Create an Amazon Bedrock client client = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34; # If you\u0026#39;ve configured a default region, you can omit this line ) # Define the model and message model_id = \u0026#34;us.anthropic.claude-3-5-haiku-20241022-v1:0\u0026#34; messages = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}]}] response = client.converse( modelId=model_id, messages=messages, ) # Print the response print(response[\u0026#39;output\u0026#39;][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;]) 3.You can also use native libraries like Python Requests:\nimport requests import os url = \u0026#34;https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\u0026#34; payload = { \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}] } ] } headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {os.environ[\u0026#39;AWS_BEARER_TOKEN_BEDROCK\u0026#39;]}\u0026#34; } response = requests.request(\u0026#34;POST\u0026#34;, url, json=payload, headers=headers) print(response.text) Bridging developer experience and enterprise security requirements As an administrator, you can enable short-term API keys to streamline user onboarding for Amazon Bedrock foundation models while ensuring a higher level of security. These keys leverage AWS Signature Version 4 and existing IAM principals, maintaining your established access controls.\nFor audit and compliance purposes, all API calls are logged in AWS CloudTrail. API keys are passed as authorization headers to API requests and are not logged.\nControlling permissions for API keys You can use Service Control Policies (SCPs) with Amazon Bedrock condition keys to customize API key generation and usage to meet your organization’s requirements.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;bedrock:CallWithBearerToken\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } You can also enforce expiration limits on long-term API keys to ensure regular rotation. The following SCP prevents creation of keys with lifespans exceeding 30 days:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:CreateServiceSpecificCredential\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:ServiceSpecificCredentialServiceName\u0026#34;: \u0026#34;bedrock.amazonaws.com\u0026#34; }, \u0026#34;NumericGreaterThanEquals\u0026#34;: { \u0026#34;iam:ServiceSpecificCredentialAgeDays\u0026#34;: \u0026#34;30\u0026#34; } } } ] } Refer to the Amazon Bedrock documentation for additional SCP examples.\nConclusion Amazon Bedrock API keys can be used in the commercial AWS regions Amazon Bedrock is available. To learn more about API keys in Amazon Bedrock, visit the API Keys documentation in the Amazon Bedrock user guide.\nGive API keys a try in the Amazon Bedrock console today and send feedback to AWS re:Post for Amazon Bedrock or through your usual AWS Support contacts.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "New features and developer experience with enhanced Amazon Location Service This blog post was written by Yasunori Kirimoto – CEO of MIERUNE\nBuilding geospatial applications requires expertise in handling geospatial data, as well as designing and developing systems. It also requires skills in collecting and managing vast amounts of geospatial data and using it effectively within the application. This process can be very labor intensive, but its complexity can be greatly reduced by leveraging Amazon Location Service.\nWith Amazon Location Service, highly accurate geospatial data can be quickly obtained from APIs, allowing developers to focus on building applications. In addition, Amazon Location Service has been updated, adding new features in addition to its previous functionality. We’ll introduce the new features of Amazon Location Service and demonstrate how to leverage them in your application.\nNew features released from Amazon Location Service The biggest change is that resource creation is no longer required. This means users no longer need to create individual resources (such as Place Indexes, Maps, and Route Calculators), but can set up an API key and immediately start using Amazon Location Service.\nIn addition, significant enhancements and new features have been added to the Maps, Places, and Routes APIs. The Maps API has been updated with additional styles, as well as the new static map capability. The Places API has been enhanced with new search and geocoding capabilities. Finally, the Routes API has been updated with new features such as Snap to Road, Waypoint Optimization, and additional travel modes.\nCreating API Keys In order to create an API Key, we can utilize the AWS Management Console, or the AWS Cloud Control API. For this example, we will use the console. Navigate to the Amazon Location Service Console, and select API keys under Manage resources. Select Create API key For the purposes of our demonstration, we will name the API Key LasVegasMaps and select the following actions:\n– GetStaticMap – GetTile – Geocode – GetPlace – SearchNearby – SearchText – CalculateIsolines – SnapToRoads\nScrolling down, we have additional options including the ability to set an Expire time, and Referers. These are optional, but we highly recommend them for production applications. NOTE: For the purposes of this demonstration we are leaving these as Default.\nSelect Create API key. Now with the API Key created, we need to retrieve the value to use in our application. Select Show API key value and copy the value into a safe location.\nNew Maps API features First, we will highlight and introduce the GetStyleDescriptor and GetStaticMap functions.\nBuilding the foundation for a map application with GetStyleDescriptor The GetStyleDescriptor function allows you to retrieve map style information and quickly build the foundation for your map application. This feature could be used for various geospatial solutions and application foundations. The new version offers expanded map styles, purpose-built for different applications—offering dark and light mode with varying levels of map detail.\nWe will demonstrate how to take advantage of these map styles using MapLibre GL JS. We’ll create a very straightforward HTML page using MapLibre GL JS and Amazon Location Service API Keys.\nBegin by creating a blank HTML page, naming it simpleMap.html, and copying the following code into the page:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service Map\u0026lt;/title\u0026gt; \u0026lt;!-- MapLibre GL CSS --\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; /* Full viewport height */ } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // Replace with your AWS region const mapApiKey = \u0026#34;\u0026lt;Your API Key\u0026gt;\u0026#34;; // Replace with your Amazon Location Service API key async function initializeMap() { // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, // HTML element ID where the map will be rendered center: [-115.1473824627421, 36.17071351509272], // Initial map center coordinates (Las Vegas) zoom: 12, // Initial zoom level style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, // Map style URL }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Now open this HTML page in a browser. You should see a map of Las Vegas, NV. The key to this map is the style URL we set previously in our code. In this URL, we’ve requested that we use the Standard style map in a light color scheme. We can add additional parameters such as political views as well.\nCreating a static map image with GetStaticMap zoom level, and image size you specify. This feature helps include map images in printed materials and media posts. There are various parameters for this feature, including the ability to overlay other data (such as points, lines and polygons). We’ve provided a basic example. Be certain to edit the URL for your AWS Region and your newly created API Key. Paste the following URL into the address bar of your web browser to display a static map image of the specified location: https://maps.geo..amazonaws.com/v2/static/map?center=-115.170,36.122\u0026amp;zoom=15\u0026amp;width=1024\u0026amp;height=1024\u0026amp;key=\nNew Places API features Next, we will highlight and introduce the SearchText and SearchNearby features\nSearching for specified POI data with SearchText The SearchText feature allows users to search for and present specified points of interest (POI) data. This feature is designed for users to quickly search for a specific location or facility. Users can send a POST request with the specified parameters and receive a response containing candidate point data. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file named searchText.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service – Search Text\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Places client const client = new amazonLocationClient.places.GeoPlacesClient( authHelper.getClientConfig() ); // Define search parameters for coffee shops const SearchTextInput = { BiasPosition: [-115.170, 36.122], // Las Vegas coordinates MaxResults: 25, QueryText: \u0026#34;Coffee Shops\u0026#34; } // Perform the search using Amazon Location Service const searchResults = await client.send( new amazonLocationClient.places.SearchTextCommand(SearchTextInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.170, 36.122], // Las Vegas coordinates zoom: 14, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add search results as a layer map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert search results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.searchTextResponseToFeatureCollection(searchResults); // Add a data source containing GeoJSON from the search results map.addSource(\u0026#34;place-index-results\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection, }); // Add a new layer to visualize the points map.addLayer({ id: \u0026#34;place-index-results\u0026#34;, type: \u0026#34;circle\u0026#34;, source: \u0026#34;place-index-results\u0026#34;, paint: { \u0026#34;circle-radius\u0026#34;: 8, \u0026#34;circle-color\u0026#34;: \u0026#34;#0080ff\u0026#34;, }, }); // Add click event listener for the search result points map.on(\u0026#39;click\u0026#39;, \u0026#39;place-index-results\u0026#39;, (e) =\u0026gt; { if (e.features.length \u0026gt; 0) { const feature = e.features[0]; const coordinates = feature.geometry.coordinates.slice(); // Create a formatted HTML string with the feature\u0026#39;s properties const properties = feature.properties; let description = \u0026#39;\u0026lt;h3\u0026gt;\u0026#39; + (properties[\u0026#39;Title\u0026#39;] || \u0026#39;Unnamed Location\u0026#39;) + \u0026#39;\u0026lt;/h3\u0026gt;\u0026#39;; description += \u0026#39;\u0026lt;p\u0026gt;Address: \u0026#39; + (properties[\u0026#39;Address.Label\u0026#39;] || \u0026#39;N/A\u0026#39;) + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; // Create and display a popup with the location information new maplibregl.Popup() .setLngLat(coordinates) .setHTML(description) .addTo(map); } }); map.on(\u0026#39;mouseenter\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;pointer\u0026#39;; }); map.on(\u0026#39;mouseleave\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;\u0026#39;; }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map with coffee shops centered around the Venetian Resort in Las Vegas, NV.\nSearching to retrieve POI data around a specified location with SearchNearby The SearchNearby function allows you to retrieve POI data near a specified location. This feature is handy for users to search for nearby stores and attractions. Users can send a POST request with the specified parameters and receive a response containing candidate point data. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file named searchNearby.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service – Search Nearby\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating map requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Places client const client = new amazonLocationClient.places.GeoPlacesClient( authHelper.getClientConfig() ); // Define search parameters for nearby casinos and hotels const SearchNearbyInput = { QueryPosition: [-115.170, 36.122], // Las Vegas coordinates MaxResults: 25, Filter: { IncludeCategories: [ \u0026#34;casino\u0026#34;, \u0026#34;hotel\u0026#34; ] } } // Perform the nearby search using Amazon Location Service const searchResults = await client.send( new amazonLocationClient.places.SearchNearbyCommand(SearchNearbyInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.170, 36.122], // Las Vegas coordinates zoom: 15, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add search results as a layer map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert search results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.searchNearbyResponseToFeatureCollection(searchResults); // Add a data source containing GeoJSON from the search results map.addSource(\u0026#34;place-index-results\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection, }); // Add a new layer to visualize the points map.addLayer({ id: \u0026#34;place-index-results\u0026#34;, type: \u0026#34;circle\u0026#34;, source: \u0026#34;place-index-results\u0026#34;, paint: { \u0026#34;circle-radius\u0026#34;: 8, \u0026#34;circle-color\u0026#34;: \u0026#34;#0080ff\u0026#34;, }, }); // Add click event listener for the search result points map.on(\u0026#39;click\u0026#39;, \u0026#39;place-index-results\u0026#39;, (e) =\u0026gt; { if (e.features.length \u0026gt; 0) { const feature = e.features[0]; const coordinates = feature.geometry.coordinates.slice(); // Create a formatted HTML string with the feature\u0026#39;s properties const properties = feature.properties; let description = \u0026#39;\u0026lt;h3\u0026gt;\u0026#39; + (properties[\u0026#39;Title\u0026#39;] || \u0026#39;Unnamed Location\u0026#39;) + \u0026#39;\u0026lt;/h3\u0026gt;\u0026#39;; description += \u0026#39;\u0026lt;p\u0026gt;Address: \u0026#39; + (properties[\u0026#39;Address.Label\u0026#39;] || \u0026#39;N/A\u0026#39;) + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; // Create and display a popup with the location information new maplibregl.Popup() .setLngLat(coordinates) .setHTML(description) .addTo(map); } }); map.on(\u0026#39;mouseenter\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;pointer\u0026#39;; }); map.on(\u0026#39;mouseleave\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;\u0026#39;; }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map showing hotel and casino locations centered around the Venetian Resort in Las Vegas, NV.\nNew Routes API features Finally, we will discuss the new CalculateIsolines and SnapToRoads functions.\nFind the reachable range from a specified location with CalculateIsolines The CalculateIsolines function can retrieve the reachable range from a specified point. Some use cases for Isolines include identifying deliverable areas and evaluating property locations. Users can send a POST request with the specified parameters and receive a response containing polygon data indicating the reachable area. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file and name it calculateIsolines.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service - Isolines\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for the Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating map requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Routes client const client = new amazonLocationClient.routes.GeoRoutesClient( authHelper.getClientConfig() ); // Define parameters for calculating isolines const IsolinesInput = { Origin: [-115.17015436843275, 36.12122662193694], // Starting point coordinates Thresholds: { Time: [ 300, 600, 900 // Time thresholds in seconds ] }, TravelMode: \u0026#34;Pedestrian\u0026#34; // Travel mode for isoline calculation } // Calculate isolines using Amazon Location Service const routeResults = await client.send( new amazonLocationClient.routes.CalculateIsolinesCommand(IsolinesInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.16766776735061, 36.12177195550658], // Map center coordinates zoom: 15, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // Add a marker at the origin point const marker = new maplibregl.Marker() .setLngLat([-115.17015436843275, 36.12122662193694]) .addTo(map) // When the map is loaded, add isolines as layers map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert isoline results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.calculateIsolinesResponseToFeatureCollection(routeResults); // Add a data source containing GeoJSON from the isoline results map.addSource(\u0026#34;isolines\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection }); // Add a fill layer to visualize the isoline areas map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-900\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 900], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#0000ff\u0026#39;, \u0026#39;fill-opacity\u0026#39;: 0.5 } }); // Add a layer for 600m (10) map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-600\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 600], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#00ff00\u0026#39;, \u0026#39;fill-opacity\u0026#39;: .5 } }); // Add a layer for 300m (5) map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-300\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 300], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#f10000\u0026#39;, \u0026#39;fill-opacity\u0026#39;: 0.5 } }); // Add an outline layer to highlight the isoline boundaries map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-outline\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;paint\u0026#39;: { \u0026#39;line-color\u0026#39;: \u0026#39;#000000\u0026#39;, \u0026#39;line-width\u0026#39;: 2 } }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map (Figure 9). This map shows walkable distances from the Venetian Resort in 5-, 15-, and 30-minute timeframes.\nObtaining location-corrected route data with SnapToRoads The SnapToRoads function allows you to snap GPS data and other location data to the nearest road and obtain line data after location correction. This feature is very useful in improving the accuracy of vehicle tracking and traffic analysis. Users can send a POST request with specified parameters and receive a response containing position-corrected line data. We will demonstrate an example of visualizing the data before and after processing on a map.\nCreate a new HTML file named snapToRoad.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service - Snap to Roads\u0026lt;/title\u0026gt; \u0026lt;!-- MapLibre GL CSS --\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; /* Full viewport height */ } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Amazon Location utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // Replace with your AWS region const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; // Replace with your API key async function initializeMap() { // Create authentication helper const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize Amazon Location Service Routes client const client = new amazonLocationClient.routes.GeoRoutesClient( authHelper.getClientConfig() ); // GPS trace coordinates const gpsTraceCoordinates = [ [-115.14564318728544, 36.09359703860663], [-115.14522142988834, 36.09368914760097], [-115.14477687479442, 36.09345887491297], [-115.14452610012599, 36.093560194978636], [-115.14432092085157, 36.09364309311755], [-115.14383077036334, 36.09360624951118], [-115.14303285096376, 36.09360624951118], [-115.14273648090104, 36.09375362383305], [-115.14218933616989, 36.09353256224662], [-115.14163079259018, 36.0936154604141], [-115.14126602943631, 36.093633882217304], [-115.14136861907319, 36.09338518751021], [-115.14126602943631, 36.093035171404196], [-115.14093546282783, 36.092906217708844], [-115.140479508885, 36.09289700672278], [-115.14033132385379, 36.09270357576062], [-115.13951060675709, 36.09278647480254], [-115.13900905742025, 36.09291542869438], [-115.13839351959763, 36.09298911653757], [-115.1378349760179, 36.092906217708844], [-115.13733342668108, 36.09301674946067], [-115.13703705661834, 36.09286016276663] ]; // Format trace points for Snap to Roads API const tracePoints = gpsTraceCoordinates.map(coord =\u0026gt; ({ Position: coord, })); // Snap to Roads API input const SnapInput = { TracePoints: tracePoints }; // Call Snap to Roads API const snapResults = await client.send( new amazonLocationClient.routes.SnapToRoadsCommand(SnapInput) ); // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.14127503567818, 36.09249839687936], // Las Vegas area zoom: 16, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add the GPS trace and snapped route map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert snap results to GeoJSON const featureCollection = amazonLocationDataConverter.snapToRoadsResponseToFeatureCollection(snapResults); // Add GPS trace source map.addSource(\u0026#34;gpsTrace\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: { type: \u0026#34;Feature\u0026#34;, geometry: { type: \u0026#34;LineString\u0026#34;, coordinates: gpsTraceCoordinates } } }); // Add snapped trace source map.addSource(\u0026#34;snappedTrace\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection }); // Add GPS trace layer map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;gpsTrace\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;gpsTrace\u0026#39;, layout: { \u0026#34;line-join\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;line-cap\u0026#34;: \u0026#34;round\u0026#34;, }, paint: { \u0026#34;line-color\u0026#34;: \u0026#34;#00b0ff\u0026#34;, \u0026#34;line-width\u0026#34;: 8, } }); // Add snapped trace layer map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;snappedTrace\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;snappedTrace\u0026#39;, layout: { \u0026#34;line-join\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;line-cap\u0026#34;: \u0026#34;round\u0026#34;, }, paint: { \u0026#34;line-color\u0026#34;: \u0026#34;#d59a9a\u0026#34;, \u0026#34;line-width\u0026#34;: 8, } }); }); } // Initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map (Figure 10), with the blue line representing a GPS trace, and the red line representing a version that is snapped to roads.\nCleanup The only Amazon Location Service resources created during this demonstration was an API Key. To delete the API Key, navigate to the Amazon Location Service Console, select the API Key we created, and select Deactivate. Confirm your decision to deactivate the key, then select Delete. Also select that you would like to bypass the standard 90-day deactivation period.\nConclusion Amazon Location Service now offers even greater flexibility with the new features. With this update, the previously required resource creation procedure is no longer necessary, and various APIs can be used by setting an API key. This allows users to quickly and smoothly build geospatial applications.\nNotable new features include GetStyleDescriptor and GetStaticMap in the Maps API, SearchText and SearchNearby in the Places API, and CalculateIsolines and SnapToRoads in the Routes API.\nIn the Maps API, GetStyleDescriptor can be used to retrieve various map styles and apply them to your application, and GetStaticMap can generate static map images based on the coordinates and zoom levels you specify. The Places API allows you to search POI data using SearchText, and SearchNearby will enable you to find POIs around a specific location. The Routes API can use CalculateIsolines to calculate reachability from a specified point and SnapToRoads to correct GPS data to obtain accurate route data.\nThese new features allow application developers to more effectively utilize geospatial data and significantly improve the user experience. Contact an AWS Representative for more information about how we can help accelerate your business.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Accelerate VMware to AWS Migration with Trianz’s Rapid Migration Offer and Concierto platform by Renuka Krishnan and Ciji Joseph on 31 OCT 2024 in AWS for VMware, Hybrid Cloud Management, Migration, Migration Acceleration Program (MAP), Migration Solutions, Partner solutions, VMware Cloud on AWS, Windows on AWS\nIntroduction In this blog post, we explore how Trianz’s Rapid Migration Offer (RMO) in collaboration with Amazon Web Services (AWS) facilitates seamless and efficient enterprise cloud adoption by leveraging the Concierto platform as the key enabler in streamlining and automating the entire migration process.\nIn the ever-evolving world of cloud computing, enterprises are constantly seeking efficient and reliable solutions to streamline their cloud adoption strategies. The VMware ecosystem has long been a cornerstone in virtualization and cloud infrastructure. With the Broadcom acquisition and increase in licensing costs, enterprises find themselves at a critical juncture to migrate and modernize their IT infrastructure swiftly and securely.\nTraditional migration methods are complex, resource intensive, and fraught with uncertainty leaving organizations grappling with lengthy timelines, cost overruns, and operational disruptions. This is where fixed-price and prescriptive migration methodologies such as RMO, come into play. By providing price predictability, expert skill set, and a proven methodology for a time-bound migration, supported by automations and tooling that can orchestrate the entire end-to-end migration journey, these solutions help simplify and accelerate the cloud journey.\nChallenges in VMware migration through traditional solutions Traditional migration methods require deep technical expertise, increasing errors and delays. Reliance on complex scripting and command-line tools can overwhelm IT professionals lacking coding skills. Limited automation necessitates manual intervention, amplifying human error. Inadequate testing capabilities and resource-intensive manual processes can lead to cost overruns and higher staffing demands. Lack of automated discovery and assessment tools in managing dependencies between applications, databases, and storage further adds complexity and error risk to the migration process. These challenges emphasize the need for a more streamlined, prescriptive, and automated approach to facilitate smoother transitions to the AWS Cloud.\nRapid Migration Offer Large-scale cloud migrations often face lengthy timelines and cost overruns due to a lack of specialized skills and tools, requiring development, hiring, or outsourcing. Migration projects are often structured on a “Time and Materials” (T\u0026amp;M) basis, leaving customers with uncertain costs and timelines, bearing the burden of delays and overruns.\nTo address these issues, the AWS Migration Acceleration Program (MAP) team created the Rapid Migration Offer (RMO), a prescriptive, fixed-price migration methodology designed to de-risk and accelerate enterprise-wide migrations to AWS. RMO offers several key benefits. 1.Cost transparency – Unlike traditional time and materials (T\u0026amp;M) projects, RMO clearly lays out the customer’s migration price-per-server upfront, providing cost predictability. 2. Simplified migration strategy – RMO provides an “easy button” for lift-and-shift portions of a migration, allowing customers to focus on modernization efforts. 3. Turnkey migration delivery – RMO is a prescriptive, end-to-end offering where consulting partners or AWS ProServe handle the entire migration process, including onboarding customers onto a managed services provider (MSP), if needed.\nTrianz has collaborated with AWS to deliver RMO as a comprehensive solution for accelerating enterprise-wide migrations to AWS. By leveraging Concierto, a hyper automated zero code SaaS platform, in conjunction with RMO’s prescriptive methodology, organizations can significantly reduce the migration cycle time. For example, Concierto recently completed a large-scale VMware and Windows migration for a customer, involving more than 1,400 VMs within 4 months, encompassing rapid discovery, automated assessment, and migration execution using the RMO framework.\nTrianz Approach to RMO and Cloud Migration Trianz’s fixed-price RMO offering leverages a team of skilled migration practitioners, the proven migration methodology that utilizes AWS migration best practices, and Concierto’s migration solution (Concierto Migrate) to deliver a highly automated and accelerated path to the cloud, across three key phases.\nAssess and mobilize This phase accelerates the discovery, assessment, and planning process from months to few days. The comprehensive approach minimizes risks, optimizes strategies, and ensures efficient and cost-effective cloud adoption for enterprises.\nAutomated assessment – The migration process starts with comprehensive IT landscape discovery, identifying all assets and dependencies. The platform automatically generates detailed assessments of IT infrastructure and applications, identifying gaps using the AWS Cloud Adoption Framework (Figure 1). This provides target cloud recommendations, costs, and licensing implications to support informed decision-making.\n*** Migration wave planning** – Migration tasks are grouped by related applications and infrastructure to create migration waves based on business priorities and dependencies. The platform intelligently creates the move groups based on application dependencies, enabling sequencing, effort estimation, and timeline scheduling for a clear execution plan (Figure 2).\nConcierto integrates landing zone creation, configuring the target cloud environment with necessary account structures, network designs, and security controls 2.Migrate and modernize This phase enables migrations with minimal disruptions, with clear visibility and control. Automated workflows slash migration time, reduce errors, and optimize post-migration performance and cost. Concierto enables scheduled migrations with near-zero downtime and continuous data sync, allowing real-time monitoring and adjustments through migration dashboards, ensuring control over every migration phase 3.Operate and optimize The operate and optimize phase helps maximize cloud investments by balancing cost-effectiveness and high performance. This includes monitoring and optimizing expenditures to ensure the AWS infrastructure is cost-effective and high performing\nWhat you get out of this Accelerated AWS migration – Accelerate bulk migrations to the cloud by 40% or more through pre-built catalogs and simplified workflows. Hybrid cloud automation and orchestration – Manage cloud and on-premises operations through a single platform with hundreds of built-in automations. Consolidated hybrid cloud operations management – Centralized lifecycle management through a user-friendly interface, eliminating the need for legacy software and reducing complexity. Reduced total cost of ownership (TCO) – Up to 30% lower TCO through streamlined processes and increased automation.\nStrategic programs and offerings In collaboration with AWS, Concierto offers a suite of programs and offerings for customers:\nFree combined assess and mobilize Zero upfront cost migrations with the Concierto MIGRATE Fixed price (per VM) Concierto MANAGE for hybrid cloud operations. Net-zero dollar licensing through the AWS ISV Migration Tooling Offer. Conclusion In this blog post, we explored how the collaboration between AWS and Trianz offers a reliable, efficient, and simplified pathway for organizations to optimize their IT investments amid the evolving VMware landscape. The Trianz RMO leveraging Concierto addresses migration challenges by removing complexity, preventing cost overruns, and minimizing manual errors. The unified platform simplifies operations and streamlines dependency management, centralizing control to enhance visibility and enable smoother transitions with optimized performance.\nAWS has significantly more services, and more features within those services, than any other cloud provider, making it faster, easier, and more cost effective to move your existing applications to the cloud and build nearly anything you can imagine. Give your Microsoft applications the infrastructure they need to drive the business outcomes you want. Visit our .NET on AWS and AWS Database blogs for additional guidance and options for your Microsoft workloads. Contact us to start your migration and modernization journey today.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop Overview — LearningHub (2HTD) This workshop guides participants to deploy and validate the core components of 2HTD-LearningHub as described in the proposal: frontend hosted on Vercel, backend serverless with AWS Lambda + API Gateway, relational data on SQL Server running in EC2 (private subnet), and media stored in S3. The workshop focuses on hands-on tasks: provisioning network (VPC/subnets), configuring EC2 + SQL Server, setting up S3 uploads with presigned URLs, deploying Lambda + API Gateway, configuring Cognito, and accessing EC2 securely via AWS Systems Manager (SSM).\nThe workshop uses a simple test environment (optionally two VPCs: one cloud VPC for LearningHub resources and an on-prem simulation VPC) to demonstrate network flows and secure access.\nFigure: Workshop diagram — VPC, EC2 (SQL Server), S3, Lambda, API Gateway, Cognito, and SSM\nLearning Objectives Understand LearningHub\u0026rsquo;s hybrid architecture (Vercel + Lambda + EC2 + S3). Provision VPCs, subnets and configure NAT / endpoints to optimize egress cost. Deploy EC2 Windows + SQL Server Express and perform basic administration via SSM. Configure an S3 bucket and perform uploads using presigned URLs from the frontend. Deploy a Lambda (Node.js) and configure API Gateway (HTTP API) to invoke it. Configure Amazon Cognito User Pool and test authenticated API flows. Core Labs Basic provisioning: create a VPC with public/private subnets, an Internet Gateway and NAT (or NAT instance for labs). Launch an EC2 Windows instance, install SQL Server Express (or use a prebuilt AMI), and configure security groups for private access. Configure SSM Agent and access EC2 with Session Manager (no public RDP). Create a test S3 bucket, attach an IAM role to Lambda, and test uploads with presigned URLs. Deploy a simple Lambda function (Node.js) and wire it to API Gateway (HTTP API). Configure a Cognito User Pool, register a test user, and call protected APIs. Observability: enable CloudWatch Logs for Lambda, inspect logs, and create a basic dashboard. References Architecture diagram: /images/architecture.png (proposal details) Sample code for presigned URL, Lambda handler, and Terraform/CDK snippets will be provided in the workshop repo. If you prefer, I can condense the labs into 4 main exercises, or expand each lab into a step-by-step checklist (commands, IAM policies, Terraform snippets).\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 — Goals Introduce myself and connect with the First Cloud Journey cohort. Gain a practical introduction to core AWS services and the Console vs CLI workflows. Planned activities this week Day Activity Start Date Completion Date Reference Material 2 - Meet fellow FCJ members and review internship guidelines 08/11/2025 08/11/2025 3 - Explore AWS service categories:\n+ Compute\n+ Storage\n+ Networking\n+ Database 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account\n- Install and configure AWS CLI (practice basic commands and configuration) 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study EC2 fundamentals: instance families, AMIs, EBS volumes; learn SSH access methods and Elastic IPs 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on: launch an EC2 instance, connect via SSH, and attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 — Outcomes Gained a clear understanding of AWS and the main service groups:\nCompute, Storage, Networking, Database, etc. Successfully registered and configured an AWS Free Tier account.\nNavigated the AWS Management Console and located commonly used services.\nInstalled and set up AWS CLI on the workstation (configured Access Key, Secret, Region).\nUsed the AWS CLI to perform introductory checks and commands (region listing, EC2 inspection, key pair management).\nPracticed managing AWS resources using both Console and CLI workflows.\nReady to proceed to Week 2 exercises (static website + RDS / S3).\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 — Objectives Learn how CloudFront (CDN), DynamoDB (NoSQL) and ElastiCache (Redis) work and when to apply them. Improve website performance using CDN and explore NoSQL patterns and caching for faster responses. Planned tasks this week Day Task Start Date Completion Date Reference Material 2 - Introduction to CDN concepts and benefits of CloudFront\n- Create a CloudFront Distribution to serve S3 website content 22/09/2025 22/09/2025 AWS Journey 3 - Configure CloudFront behaviors and cache policies\n- Test site access via CloudFront URL\n- Perform invalidation to refresh cached content 23/09/2025 23/09/2025 AWS Journey 4 - Introduction to DynamoDB (NoSQL)\n- Create DynamoDB tables (Users, Products, etc.)\n- Perform CRUD operations using Console 24/09/2025 24/09/2025 AWS Journey 5 - Query and operate DynamoDB via AWS CLI\n- Write small scripts to insert and read data from tables 25/09/2025 25/09/2025 AWS Journey 6 - Explore ElastiCache (Redis \u0026amp; Memcached)\n- Create a basic Redis cluster\n- Test connecting from EC2 to read/write cached data 26/09/2025 26/09/2025 AWS Journey Week 3 — Outcomes Understood the role of a CDN (CloudFront) in improving website performance. Created and configured a CloudFront Distribution for the S3-hosted website and verified content delivery via the CDN endpoint. Created DynamoDB tables and practiced CRUD operations using Console and CLI. Deployed a simple ElastiCache Redis cluster and validated connectivity from an EC2 client. Integrated caching + NoSQL patterns into the static website model to improve access speed. "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand the Migration process (moving systems to AWS) and Disaster Recovery (post-incident recovery). Practice AWS Database Migration Service (DMS) and Elastic Disaster Recovery (EDR). Learn backup and restore techniques and prepare a basic emergency plan for infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor) - Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey 3 - Practice creating a DMS Replication Instance - Configure source (on-premises) and target (RDS) - Perform a test data migration 30/09/2025 30/09/2025 AWS Journey 4 - Introduction to Elastic Disaster Recovery (EDR) - Learn to set up replication servers and recovery instances 01/10/2025 01/10/2025 AWS Journey 5 - Perform a failover simulation: stop the primary EC2 and boot recovery instance from EDR - Evaluate recovery time objectives (RTO) and recovery point objectives (RPO) 02/10/2025 02/10/2025 AWS Journey 6 - Draft a basic Disaster Recovery plan (backup, restore, failover) - Write a summary document for the Migration + DR process - Week 4 knowledge summary 03/10/2025 03/10/2025 AWS Journey Week 4 Achievements: Understood the end-to-end Migration process for applications and databases to AWS.\nSuccessfully created and configured a DMS Replication Instance and executed a sample data migration.\nSet up Elastic Disaster Recovery (EDR) for basic protection against incidents.\nSuccessfully simulated a failover and validated recovery procedures.\nCompleted a basic Disaster Recovery plan to prepare for Week 5 (Infrastructure as Code \u0026amp; Systems Manager).\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Get hands-on with Infrastructure as Code (IaC) using AWS CloudFormation and AWS CDK. Manage system resources centrally with AWS Systems Manager (SSM). Understand how to automate deployment and operational tasks for AWS infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce Infrastructure as Code (IaC) concepts and benefits vs manual deployment - Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey 3 - Write a CloudFormation template to deploy an S3 bucket and EC2 instance - Create, update, and delete stacks via the AWS Console 07/10/2025 07/10/2025 AWS Journey 4 - Introduce AWS CDK (Cloud Development Kit) - Install AWS CDK, create a CDK project in Python or TypeScript - Write CDK code to deploy an EC2 instance 08/10/2025 08/10/2025 AWS Journey 5 - Introduce AWS Systems Manager (SSM) and core features: Parameter Store, Run Command, Automation, Session Manager - Create Parameter Store entries for configuration variables 09/10/2025 09/10/2025 AWS Journey 6 - Practice creating an Automation Document in SSM to start/stop EC2 instances - Test Session Manager (access EC2 without SSH keys) - Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey Week 5 Achievements: Understood IaC concepts and benefits for infrastructure management.\nCreated CloudFormation templates to deploy S3 \u0026amp; EC2 and successfully managed stacks.\nGot hands-on with AWS CDK and deployed resources using code.\nLearned to use AWS Systems Manager to manage configuration, run commands, and automate tasks.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand security concepts and cost management in AWS. Practice IAM Policies, KMS, and Secrets Manager to secure resources. Monitor, analyze, and alert on usage costs using Billing \u0026amp; Cost Explorer. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review basic IAM concepts - Learn advanced IAM Policy (JSON structure, Effect, Action, Resource, Condition) - Create custom policies and attach to user/group 13/10/2025 13/10/2025 AWS Journey 3 - Introduce AWS Key Management Service (KMS) - Create a Customer Managed Key (CMK) and test file encryption/decryption - Apply KMS to encrypt S3 buckets or EBS volumes 14/10/2025 14/10/2025 AWS Journey 4 - Get familiar with AWS Secrets Manager - Create a secret to store database credentials - Write a small Lambda script to read secrets from Secrets Manager 15/10/2025 15/10/2025 AWS Journey 5 - Explore the AWS Billing Dashboard and Cost Explorer - View costs by service, region and time - Configure Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey 6 - Create AWS Budgets and configure email alerts - Write a weekly cost summary and propose optimizations (stop unused EC2, cleanup EBS, reduce log retention, etc.) - Week 6 summary 17/10/2025 17/10/2025 AWS Journey Week 6 Achievements: Mastered creating and applying advanced IAM Policies to control resource access.\nUnderstood KMS encryption mechanisms and applied them to S3 and EBS.\nDeployed Secrets Manager to protect sensitive information, accessible from Lambda.\nMonitored costs and configured alerts with Cost Explorer and Budgets.\nLearned how to analyze costs, recommend optimizations, and maintain security in parallel.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand and implement High Availability (HA) and Auto Scaling on AWS. Configure Elastic Load Balancers (ELB) and Auto Scaling Groups (ASG) for EC2. Use SQS/SNS for queueing and notifications. Analyze network activity using VPC Flow Logs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study High Availability, Fault Tolerance and Elasticity concepts - Introduction to Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey 3 - Practice creating an Auto Scaling Group for EC2 instances - Configure launch template, scaling policy and target tracking 21/10/2025 21/10/2025 AWS Journey 4 - Create and configure an Application Load Balancer (ALB) - Attach ALB to ASG for traffic distribution - Test web access through ALB DNS 22/10/2025 22/10/2025 AWS Journey 5 - Get familiar with Amazon SQS and SNS - Create SQS queue, SNS topic and subscription - Send and receive notifications between components 23/10/2025 23/10/2025 AWS Journey 6 - Enable VPC Flow Logs to monitor network traffic - Analyze logs in CloudWatch Logs - Week summary: reliability \u0026amp; scaling 24/10/2025 24/10/2025 AWS Journey Week 7 Achievements: Understood High Availability models and how to maintain uptime during incidents.\nSuccessfully deployed Auto Scaling Group + Load Balancer to automatically scale EC2.\nConfigured SQS/SNS to communicate and send notifications between AWS services.\nEnabled and read VPC Flow Logs, analyzed network traffic using CloudWatch.\nImproved system resilience, ensuring higher availability and stable performance.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Review and consolidate core knowledge areas in the AWS Well-Architected Framework: security, reliability, performance, and cost optimization. Become proficient with key services: EC2, S3, IAM, RDS, VPC, Lambda, CloudWatch, CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Well-Architected Framework, 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization - Identify the role and importance of each pillar 27/10/2025 27/10/2025 AWS Journey 3 - Review Secure Architectures → IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 28/10/2025 28/10/2025 AWS Journey 4 - Review Resilient Architectures → Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey 5 - Review High-Performing \u0026amp; Cost-Optimized Architectures → EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, Cost Explorer, Budgets, Savings Plans, Storage Tiering 30/10/2025 30/10/2025 AWS Journey 6 - Final practical: + Build a sample architecture combining EC2, S3, RDS, IAM, VPC, CloudFront, Lambda, CloudWatch + Evaluate against the 5 Well-Architected pillars + Write weekly summary 31/10/2025 31/10/2025 AWS Journey Week 8 Achievements: Deepened understanding and systematized the AWS Well-Architected Framework.\nReinforced four core architecture focus areas: Security, Reliability, Performance, Cost Optimization.\nPracticed complete infrastructure design and self-evaluation against AWS standards.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Get familiar with and practice Data \u0026amp; Analytics services on AWS. Understand the data ingestion, storage, processing and analytics pipeline on the cloud. Use AWS tools to build a Data Lake and create BI dashboards. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce the Data \u0026amp; Analytics ecosystem on AWS - Understand Data Lake concepts, ETL pipelines, and how to connect data from multiple sources 03/11/2025 03/11/2025 AWS Journey 3 - Build a Data Lake on Amazon S3 - Design folder structure and access controls - Configure AWS Glue Crawler to detect data schema 04/11/2025 04/11/2025 AWS Journey 4 - Practice querying the Data Lake with AWS Athena - Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey 5 - Introduction and hands-on with Amazon QuickSight - Connect QuickSight to Athena for visualizations - Create a simple dashboard with charts and summary tables 06/11/2025 06/11/2025 AWS Journey 6 - Review \u0026amp; consolidate the week: + Data ingestion → processing → analytics on AWS + Compare Glue, Athena, QuickSight with traditional tools + Write a summary report of exercises 07/11/2025 07/11/2025 AWS Journey Week 9 Achievements: Understood how to build and manage a Data Lake using S3.\nPracticed ingesting, cataloging and querying data with Glue and Athena.\nCreated a BI dashboard for visual analysis using QuickSight.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and core AWS services\nWeek 2: Build a static website with S3 and connect to an RDS database\nWeek 3: Performance optimization with CloudFront, DynamoDB and ElastiCache\nWeek 4: Migration and Disaster Recovery with DMS and EDR\nWeek 5: Infrastructure as Code with CloudFormation, CDK and Systems Manager\nWeek 6: Security and Cost Management: IAM Policy, KMS, Secrets Manager, Billing Dashboard and AWS Budgets\nWeek 7: Improve system scalability: Auto Scaling, Load Balancer, SQS/SNS, and VPC Flow Logs\nWeek 8: Review AWS Well-Architected Framework and reinforce core service knowledge\nWeek 9: Data \u0026amp; Analytics practice: Data Lake with S3, Glue, Athena and QuickSight\nWeek 10: AI/ML practice: SageMaker, Rekognition, Comprehend and Kendra\nWeek 11: Modernization \u0026amp; Serverless Architecture with Lambda, API Gateway, DynamoDB, Cognito and SAM\nWeek 12: Final project - Consolidate knowledge and build a complete AWS project\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.3-cognito-api-gateway/5.3.1-create-andconfigue-cognito/",
	"title": "Create and configure Cognito &amp; API Gateway",
	"tags": [],
	"description": "",
	"content": "Purpose This guide shows step-by-step how to create and configure Amazon Cognito (User Pool + App Client, optional Identity Pool) and an Amazon API Gateway HTTP API that uses Cognito as a JWT authorizer. It includes console steps, AWS CLI/PowerShell commands, IAM notes and examples for testing.\nArchitecture assumption: frontend (Vercel) calls API Gateway (HTTP API) -\u0026gt; Gateway authorizer validates Cognito JWTs -\u0026gt; routes to Lambda (business logic). SQL Server runs on EC2 in a private subnet and is accessed by Lambda when needed.\n1. Create a Cognito User Pool (Console) Sign in to AWS Console -\u0026gt; Amazon Cognito -\u0026gt; \u0026ldquo;Manage User Pools\u0026rdquo; -\u0026gt; \u0026ldquo;Create a user pool\u0026rdquo;. Choose a name, e.g. learninghub-userpool. Choose Review defaults or Step through settings for custom options. Recommended custom settings: Attributes: keep email as required; add name if you need display names. Policies: password strength as required for your environment. MFA \u0026amp; verifications: Email verification enabled for workshop demos (optional). App clients: create at least one App client with no secret if used by SPA or mobile (uncheck \u0026ldquo;Generate client secret\u0026rdquo;). App client callback URLs: add your frontend URL(s) (e.g., https://learninghub.example.com) for Hosted UI flows. Save pool. Notes:\nFor SPAs use an App client without a secret. For backend-to-backend you may use a client secret. Hosted UI: you can configure a domain under \u0026ldquo;App integration -\u0026gt; Domain name\u0026rdquo; to use Cognito\u0026rsquo;s Hosted UI. 2. Create an App Client (Console + CLI) Console: In User Pool -\u0026gt; App clients -\u0026gt; \u0026ldquo;Add an app client\u0026rdquo;. Name it like learninghub-web-client. Uncheck \u0026ldquo;Generate client secret\u0026rdquo; for browser-based apps.\nAWS CLI (PowerShell):\n# create app client (no secret) aws cognito-idp create-user-pool-client --user-pool-id \u0026lt;USER_POOL_ID\u0026gt; --client-name learninghub-web-client --no-generate-secret --output json Record the ClientId and UserPoolId for later.\n3. (Optional) Configure a Hosted UI / Domain In User Pool -\u0026gt; App integration -\u0026gt; Domain name, create a Cognito domain (e.g. learninghub-demo-\u0026lt;suffix\u0026gt;). Configure callback and sign-out URLs under App client settings. Enable OAuth flows needed (Authorization code grant or Implicit). For SPAs you may use implicit (but Authorization code grant + PKCE is recommended). 4. Create a Cognito Identity Pool (optional) Use an identity pool when you need AWS temporary credentials (e.g., direct S3 access from client). Steps:\nConsole: Cognito -\u0026gt; Federated Identities -\u0026gt; Create identity pool -\u0026gt; enable \u0026ldquo;Authenticated identities\u0026rdquo; -\u0026gt; choose roles for authenticated/unauthenticated. CLI example (PowerShell):\naws cognito-identity create-identity-pool --identity-pool-name learninghub-identitypool --allow-unauthenticated-identities false --cognito-identity-providers ProviderName=cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt;,ClientId=\u0026lt;CLIENT_ID\u0026gt; You\u0026rsquo;ll receive an IdentityPoolId; configure IAM roles and trust policies to allow the pool to assume roles.\n5. Create IAM roles for Cognito (Identity Pool) and Lambda If you use Identity Pool, create two roles: CognitoAuthRole (for authenticated users) and CognitoUnauthRole (for unauthenticated, if enabled). Attach minimal policies (S3 GetObject/PutObject if you use direct S3 uploads).\nLambda execution role: create an IAM role that allows Lambda to access S3, Secrets Manager, EC2 (via SSM) or other services required by your backend.\nExample trust policy for a role assumed by Cognito identity pool (snippet):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;Federated\u0026#34;: \u0026#34;cognito-identity.amazonaws.com\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: {\u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: \u0026#34;\u0026lt;IDENTITY_POOL_ID\u0026gt;\u0026#34;}, \u0026#34;ForAnyValue:StringLike\u0026#34;: {\u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;authenticated\u0026#34;} } } ] } 6. Create API Gateway (HTTP API) and configure Cognito Authorizer We use HTTP API (faster, lower-cost). Configure a JWT authorizer that validates Cognito tokens.\nConsole steps API Gateway -\u0026gt; HTTP APIs -\u0026gt; Create -\u0026gt; Build. Select \u0026ldquo;Add integration\u0026rdquo; -\u0026gt; Lambda -\u0026gt; choose your Lambda function (or create placeholder). Create routes (e.g., GET /courses, POST /exams). Attach integration. Under \u0026ldquo;Authorization\u0026rdquo; -\u0026gt; Add authorizer -\u0026gt; choose JWT. Fill these fields: Identity provider: choose Cognito. Issuer: https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt; Audience: your App Client ID (client_id) — for ID tokens use client_id as audience. Some setups use the user pool\u0026rsquo;s aud claim. Attach the JWT authorizer to routes that require authentication. Deploy (Auto-deploy for HTTP API is default). Note the invoke URL (e.g., https://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com). CLI example (create authorizer) # create an HTTP API $api=$(aws apigatewayv2 create-api --name learninghub-api --protocol-type HTTP --target arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:function:YourFunction --output json) $apiId=$(echo $api | ConvertFrom-Json).ApiId # create JWT authorizer aws apigatewayv2 create-authorizer --api-id $apiId --authorizer-type JWT --name CognitoJWTAuth --identity-source \u0026#34;$request.header.Authorization\u0026#34; --jwt-configuration \u0026#34;{\\\u0026#34;Issuer\\\u0026#34;:\\\u0026#34;https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt;\\\u0026#34;,\\\u0026#34;Audience\\\u0026#34;:[\\\u0026#34;\u0026lt;CLIENT_ID\u0026gt;\\\u0026#34;]}\u0026#34; Attach the authorizer to a route (replace routeId as necessary):\naws apigatewayv2 update-route --api-id $apiId --route-key \u0026#34;GET /courses\u0026#34; --authorization-type JWT --authorizer-id \u0026lt;AUTHORZER_ID\u0026gt; Notes:\nHTTP API expects JWT Authorization header: Authorization: Bearer \u0026lt;id_token\u0026gt;. Use id_token for user info (claims). Access tokens may also be acceptable depending on configuration. 7. API Gateway -\u0026gt; Lambda permissions Add permission for API Gateway to invoke the Lambda:\naws lambda add-permission --function-name YourFunction --statement-id apigw-invoke --action lambda:InvokeFunction --principal apigateway.amazonaws.com --source-arn arn:aws:execute-api:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:${apiId}/*/*/* 8. CORS If your frontend is hosted on https://learninghub.example.com, add CORS to your routes:\nFor HTTP API you can configure CORS in console or set Access-Control-Allow-Origin header in integration responses. Example allowed origins: https://learninghub.example.com or * for quick demos (not recommended for production). 9. Test flow (Sign-up, Sign-in, call API) Create a test user in the User Pool (Console -\u0026gt; Users -\u0026gt; Create user) or use self-sign-up if enabled. Use Hosted UI or AWS CLI to authenticate and get tokens. CLI example (AdminInitiateAuth) to get tokens (PowerShell):\naws cognito-idp admin-initiate-auth --user-pool-id \u0026lt;USER_POOL_ID\u0026gt; --client-id \u0026lt;CLIENT_ID\u0026gt; --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME=\u0026#34;testuser\u0026#34;,PASSWORD=\u0026#34;P@ssw0rd\u0026#34; --output json Response contains AuthenticationResult.IdToken and AccessToken.\nCall API with bearer token: $token = \u0026#34;\u0026lt;ID_TOKEN_FROM_AUTH\u0026gt;\u0026#34; Invoke-RestMethod -Method Get -Uri \u0026#34;https://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/courses\u0026#34; -Headers @{ Authorization = \u0026#34;Bearer $token\u0026#34; } If you receive 401: check authorizer configuration (issuer/audience), ensure token is not expired and your route has the authorizer attached.\n10. Troubleshooting tips 401 Unauthorized: verify Issuer URL exactly matches https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt; and Audience contains the client id used to get the token. 403 Forbidden when invoking Lambda: check Lambda resource policy and add-permission invocation. Token issues: verify token type (ID token vs Access token) and its claims using jwt.io. CORS errors: ensure API returns proper Access-Control-Allow-Origin header. 11. Security considerations Use Authorization Code Grant + PKCE for SPAs to avoid implicit flow security issues. Do not embed client secrets in browser apps. Scope Resource ARNs in IAM policies in production. Set appropriate token expiration and rotation policies. "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "Overview This page lists the prerequisites to run the workshop and to follow the hands-on exercises for the 2HTD-LearningHub project. The content below covers required AWS permissions, recommended region, AWS services used, a minimal workshop IAM policy example, and PowerShell steps for Windows users.\nAWS account \u0026amp; permissions An AWS account with permission to create and manage resources used by the workshop (CloudFormation, EC2, VPC, S3, Lambda, API Gateway, Cognito, IAM, CloudWatch, SSM). For workshops: use a sandbox account and create an IAM user for yourself. Assign AdministratorAccess if you prefer a frictionless experience during the lab. For real environments: follow the least-privilege principle. Use the minimal IAM policy below as a starting point and scope resources (ARNs) before applying in production. Quick verification after configuring the AWS CLI:\naws configure # enter Access Key, Secret, default region (e.g. us-east-1) and output format aws sts get-caller-identity # verify credentials and account Region Use us-east-1 (N. Virginia) for the workshop demos and CloudFormation examples. If you choose another region, adjust resource names and template URLs accordingly. Required AWS services Amazon EC2 (Windows + SQL Server as the project DB) Amazon S3 (media and artifact storage) AWS Lambda (backend functions) Amazon API Gateway (HTTP API) Amazon Cognito (authentication) AWS IAM (roles and policies) Amazon CloudWatch (logs \u0026amp; metrics) AWS Systems Manager (Session Manager for EC2 access) AWS CloudFormation (or Terraform/CDK) for provisioning Route 53 (optional for DNS during demos) Local tools (recommended) Git (clone repository) Node.js (LTS, e.g. 18+) and npm (build Lambda code) AWS CLI v2 (configure with aws configure) AWS Session Manager plugin (optional, to enable aws ssm start-session integration) SQL client: SQL Server Management Studio (SSMS) or Azure Data Studio (connect to SQL Server on EC2) PowerShell (Windows) or a POSIX shell (Linux/macOS) Docker (optional — only if building containerized Lambdas) Minimal IAM policy (workshop example) This example covers common actions used by provisioning templates and deployment flows in this workshop. Review and scope resource ARNs before using in production.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;apigateway:POST\u0026#34;, \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:SendCommand\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Notes:\nThe policy above is suitable for a lab/sandbox account. Replace \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; with specific ARNs to narrow scope in production. Some CloudFormation templates create IAM roles and require CAPABILITY_NAMED_IAM when deploying; the iam:PassRole action is commonly necessary. PowerShell (Windows) quick-start Use the following PowerShell snippets to install the AWS CLI, configure credentials, verify access, and run common workshop commands.\nInstall AWS CLI v2 (download \u0026amp; install MSI):\n# Download installer and run Invoke-WebRequest -Uri \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.msi\u0026#34; -OutFile \u0026#34;$env:TEMP\\AWSCLIV2.msi\u0026#34; Start-Process msiexec.exe -Wait -ArgumentList \u0026#34;/i $env:TEMP\\AWSCLIV2.msi /qn\u0026#34; Configure AWS CLI and verify identity:\naws configure aws sts get-caller-identity Start an SSM Session (example):\n# Replace with your EC2 instance id $instanceId = \u0026#39;i-0123456789abcdef0\u0026#39; aws ssm start-session --target $instanceId Deploy a CloudFormation template:\naws cloudformation deploy --template-file .\\cloudformation\\stack.yaml --stack-name MyWorkshopStack --capabilities CAPABILITY_NAMED_IAM Upload artifacts to S3 (example):\naws s3 mb s3://my-workshop-artifacts-$(Get-Random -Maximum 99999) aws s3 cp .\\lambda\\package.zip s3://my-workshop-artifacts-12345/ Local setup steps (recap) Install the tools listed above. Configure the AWS CLI with your IAM user: aws configure (set default region to us-east-1). Verify access: aws sts get-caller-identity. (Optional) Create an S3 bucket to store deployment artifacts. Notes on EC2 \u0026amp; SSM The project stores relational data on an EC2 instance running SQL Server in a private subnet. For administration we rely on AWS Systems Manager (SSM) Session Manager instead of RDP. Ensure your IAM user has SSM permissions (or use AdministratorAccess in a lab account). Clean-up guidance If you deploy using CloudFormation or scripts, delete the stack and any created S3 buckets when finished to avoid ongoing charges. If you want, I can scope the minimal IAM policy to exact ARNs for your account, or produce a PowerShell script that automates AWS CLI install + aws configure with prompts.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/2-proposal/",
	"title": "Proposal 2HTD-LearningHub",
	"tags": [],
	"description": "",
	"content": "1. Project Summary 2HTD-LearningHub is a web platform for studying, practice, exam creation and online testing. The goal is to provide a one-stop experience for students and instructors: question bank, practice exercises, mock exams and reporting.\nThe current release removes the live-class component; instead the product focuses on authoring (exam creation), submissions, automated grading and content distribution.\nThe architecture is hybrid: frontend hosted on Vercel; backend business logic runs on AWS Lambda (invoked via API Gateway); relational data is stored in SQL Server running on Amazon EC2 in a private subnet; media and documents are stored in Amazon S3.\nThe design emphasizes security and operability: network isolation with VPC/subnets, access control with IAM and Cognito, observability via CloudWatch, and EC2 administration through SSM rather than RDP.\n2. Problem Statement Learners and instructors currently rely on multiple disconnected tools (quiz builders, video calls, storage), causing fragmented workflows and management overhead. Instructors need more efficient, controlled ways to author, review and distribute exam content. Systems can degrade during peak events (mock exams) and struggle to keep latency low for all users. Real-time notifications and reporting are limited for course management and student tracking. Exam content and user data must be protected against leaks and abuse. 3. Solution \u0026amp; Architecture The solution maps to the architecture diagram and includes:\nFrontend: Vercel (hosting SPA/static), DNS managed in Route 53. Authentication: Amazon Cognito for user pools (signup/login) and token-based authentication. API layer: Amazon API Gateway (HTTP API) as the public endpoint, routing to AWS Lambda (proxy). Backend: AWS Lambda (Node.js) for business logic. Database: Amazon EC2 (Windows + SQL Server Express) in a private subnet as the primary datastore for users, courses, quizzes and results. Storage: Amazon S3 for media and large files; the database stores metadata and S3 links. Network: VPC with public/private subnets, NAT Gateway and Internet Gateway; EC2 sits in private subnet; Lambda may run in VPC when DB access is required. Operations \u0026amp; Security: IAM roles, CloudWatch logs, and AWS Systems Manager (SSM) for accessing EC2 without RDP. Architecture Diagram Figure 1: 2HTD-LearningHub — Frontend (Vercel), API Gateway → Lambda, EC2 (SQL Server) in private subnet, S3 for storage\nAWS Services Used Amazon EC2 (Windows + SQL Server Express): Run SQL Server (SQLEXPRESS) as the primary database for the system (users, courses, quizzes, tests, results).\nAmazon S3: Store lecture files, videos, PDFs and images; the database stores metadata and S3 links.\nAWS Lambda: Run backend Node.js processes (API handlers, background tasks).\nAmazon API Gateway (HTTP API): Public HTTPS endpoint for the frontend (Vercel) to call into the backend.\nAmazon Cognito: User sign-up / sign-in, email verification, provides AccessToken/IdToken.\nAWS IAM: Permissions for Lambda to access S3, EC2, SSM.\nAmazon CloudWatch Logs: Store logs from Lambda and API Gateway for debugging and monitoring.\nAWS Systems Manager (SSM): Session Manager to access EC2 without RDP and run PowerShell/sqlcmd.\nAmazon Route 53: DNS for pointing to Vercel deployment.\nVPC, Subnets, NAT Gateway, Internet Gateway: Network isolation and secure access to resources.\nComponent Design VPC: learninghub-vpc with public \u0026amp; private subnets. EC2 DB in private subnet; NAT Gateway for outbound from private. EC2 (DB): Windows Server + SQL Server Express on EC2 instance (private), storing primary data. Backups via snapshots and maintenance policies. Lambda (Backend): Handle API requests from API Gateway; may be placed in VPC for DB access. API Gateway: HTTP API routing to Lambda (proxy). Enable CORS for the frontend. S3: Store files and provide presigned URLs for uploads/downloads. Cognito: User Pool for authentication and email/OTP verification. IAM \u0026amp; Roles: following least-privilege principle. SSM: Use Session Manager to connect to EC2, run sqlcmd or PowerShell for DB administration. CloudWatch: Collect logs and metrics; dashboards and alerts for operations. 4. Technical Implementation IaC \u0026amp; Provisioning: Use Terraform or AWS CDK to provision VPC, subnets, NAT/IGW, EC2 (Windows + SQL Server), S3 bucket, API Gateway, Lambda, Cognito, IAM roles, SSM and CloudWatch. CI/CD: GitHub Actions to build \u0026amp; deploy frontend (Vercel), deploy Lambda packages/containers, and apply IaC changes to staging/production. API \u0026amp; Data: API Gateway (HTTP API) → Lambda (Node.js) handles CRUD for courses/quizzes/tests; Lambda connects to EC2 SQL Server on the private network. EC2 database is managed via SSM for migrations and backups. File Storage: Teacher upload flow — backend issues presigned URL → frontend uploads directly to S3; DB stores metadata and link. Security \u0026amp; Operations: Least-privilege IAM, KMS encryption as needed, Secrets Manager/SSM Parameter Store for credentials; CloudWatch logs \u0026amp; alarms, backup snapshots for EC2, and runbooks for incident response. AWS Systems Manager (SSM): Use Session Manager to access and administer EC2 instances securely (run PowerShell/sqlcmd for migrations, connectivity checks, backups/restores and troubleshooting) without opening extra network ports. 5. Timeline \u0026amp; Milestones Week 1-2: Detailed design, data model and IaC scaffold; provision EC2 DB. Week 3-4: Implement Cognito, API Gateway + Lambda, test DB connectivity via SSM. Week 5-6: Implement authoring, submission and grading; integrate S3 upload flow. Week 7: System testing, backups and security review. Week 8: Load testing, observability dashboards, documentation and handover. 6. Budget Estimation AWS Service Key Billing Factors Estimated Cost (USD) EC2 + EBS (Database) Windows + SQL Server Express (instance + EBS storage, snapshots) 23.00 Amazon S3 Storage (GB) and requests (PUT/GET) 1.30 AWS Lambda Invocations and compute (low starter estimate) 1.00 Amazon API Gateway (HTTP API) Request charges (per-request pricing) 2.50 Amazon CloudWatch Logs Log ingestion \u0026amp; storage for Lambda/API 1.50 NAT Gateway Hourly + data processing (egress) — often the largest cost driver 30.00 Route 53 Hosted zone + DNS queries 1.00 Amazon Cognito + IAM + SSM Authentication, IAM operations, SSM Session Manager (no direct charge) 0.00 TOTAL ≈ 60.00 Note: Actual costs depend on traffic, region and configuration; costs can be reduced further by turning off dev/test environments or optimizing egress.\n7. Risk Assessment Risk Impact Mitigation EC2 / egress costs High Monitor usage, choose right instance size, schedule/auto-stop dev instances. Data leaks / credentials exposure Very High Use Secrets Manager, KMS, least-privilege IAM, periodic pentests. Lambda ↔ EC2 connectivity issues Medium Proper security groups, test in staging, consider a proxy if needed. DB backup/recovery failures High Automated snapshots, tested restore procedures. 8. Expected Outcomes An integrated platform for studying, practice, exam creation and online testing that reduces tool fragmentation for learners and instructors. Automation of authoring, distribution and grading to improve instructor productivity. A stable, secure, and scalable infrastructure; EC2-hosted SQL Server ensures compatibility with existing database requirements. S3-based storage for large files to reduce DB load and simplify backups; Lambda reduces operational overhead for business logic. Observability (CloudWatch) and logging to support troubleshooting and performance monitoring. "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Get familiar with and practice AI/ML services in the AWS ecosystem. Understand the training, deployment and consumption lifecycle for Machine Learning models. Hands-on with SageMaker, Rekognition, Comprehend, and Kendra. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AI/ML on AWS - Learn ML-supporting services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey 3 - Hands-on with Amazon SageMaker: + Create a Notebook Instance + Train a simple model (Linear Regression / Image Classification) + Deploy an endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey 4 - Explore Amazon Rekognition - Demo face \u0026amp; object recognition in images/videos - Integrate Rekognition API into a small web app 12/11/2025 12/11/2025 AWS Journey 5 - Practice Amazon Comprehend (NLP) - Try Amazon Kendra (contextual search) - Compare strengths and limitations of each service 13/11/2025 13/11/2025 AWS Journey 6 - Week wrap-up: + ML development lifecycle on AWS + Real-world AI/ML applications + Write a summary report and next-step recommendations 14/11/2025 14/11/2025 AWS Journey Week 10 Achievements: Understood the AWS AI/ML ecosystem and its core services.\nSuccessfully trained and deployed a basic ML model on SageMaker.\nApplied Rekognition, Comprehend, and Kendra to practical tasks.\nLearned the end-to-end ML training → deploy → integrate workflow on AWS.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn and practice Modernization \u0026amp; Serverless Architecture on AWS. Build, deploy and manage Serverless applications using Lambda, API Gateway, DynamoDB, Cognito and SAM. Understand how to split a monolith into microservices to improve scalability and maintainability. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce Modernization and Serverless concepts - Compare monolithic vs microservices architectures - Analyze benefits of moving to serverless 17/11/2025 17/11/2025 AWS Journey 3 - Hands-on with AWS Lambda: create functions, configure triggers, view logs in CloudWatch - Implement basic API logic with Lambda 18/11/2025 18/11/2025 AWS Journey 4 - Integrate API Gateway with Lambda to build REST APIs - Connect data with DynamoDB (CRUD operations) - Test APIs using Postman 19/11/2025 19/11/2025 AWS Journey 5 - Configure Cognito for user authentication (user pool, tokens) - Integrate Cognito authentication with API Gateway - Manage access via IAM Roles 20/11/2025 20/11/2025 AWS Journey 6 - Deploy the full Serverless application using AWS SAM (Serverless Application Model) - Test, collect logs and optimize performance - Week summary and report 21/11/2025 21/11/2025 AWS Journey Week 11 Achievements: Understood Serverless Architecture and benefits in cost, performance and scalability.\nDeployed a complete Serverless app using Lambda, API Gateway, DynamoDB and Cognito.\nLearned to use AWS SAM for managing and deploying serverless infrastructure automatically.\nPrepared the platform for Week 12 (Final summary \u0026amp; capstone project).\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Consolidate all knowledge gained during the previous 11 weeks. Build a final real-world project on AWS that integrates multiple services. Review and evaluate understanding and ability to apply AWS in practical problems. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for final project 24/11/2025 24/11/2025 AWS Journey 3 - Start project implementation: + Design VPC, subnets, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 AWS Journey 4 - Continue implementation: + Build backend with Lambda/API Gateway or EC2 (depending on architecture) + Connect databases and process data + Integrate CloudWatch logging 26/11/2025 26/11/2025 AWS Journey 5 - Finalize project: + Add Cognito authentication if needed + Complete CI/CD pipeline (CodePipeline/CodeBuild) + Perform end-to-end system tests 27/11/2025 27/11/2025 AWS Journey 6 - Write final report - Prepare presentation (architecture, reasons for service choices, cost, security) - Summarize learning journey and self-assess readiness for projects 28/11/2025 28/11/2025 AWS Journey Week 12 Achievements: Completed the final AWS project integrating multiple core services.\nDesigned, deployed, optimized and operated a practical system end-to-end.\nMastered the cloud application development lifecycle from design to production.\nFinished the training roadmap and prepared to apply skills in larger real-world projects.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.3-cognito-api-gateway/",
	"title": "Cognito + API Gateway",
	"tags": [],
	"description": "",
	"content": "Overview\nThis lab shows how to secure HTTP APIs using Amazon Cognito and API Gateway (HTTP API). Students will create a Cognito User Pool, an App Client, and configure a JWT authorizer in API Gateway so Lambda-backed endpoints require a valid Cognito token.\nPrerequisites\nAWS account with permission to create Cognito, API Gateway, Lambda, IAM roles. AWS CLI or Console access. A simple Lambda or HTTP backend (the workshop provides a sample handler). Learning objectives\nCreate and configure an Amazon Cognito User Pool and App Client. Configure an API Gateway HTTP API with JWT authorizer pointing to the Cognito User Pool. Protect endpoints and test access using Cognito tokens. Integrate login flow from frontend to call protected APIs. Lab steps (high level)\nCreate a Cognito User Pool Use the Console or AWS CLI to create a User Pool with email as username. Enable sign-up and create a test user. Create an App Client Create an App Client (no client secret for single-page apps) and configure callback/logout URLs for the frontend. Configure a domain (optional) Use a Cognito hosted domain or bring your own via Route 53. Deploy or identify your API backend Ensure Lambda function(s) exist and are callable by API Gateway. Create an API Gateway (HTTP API) Create HTTP API and add an integration to the Lambda backend. Add a route (e.g. POST /upload or GET /items). Add JWT Authorizer In API Gateway, create a JWT authorizer referencing the Cognito User Pool issuer and audience (App Client ID). Protect the route using the JWT authorizer. Test authentication flow Use the Cognito hosted UI to sign in and obtain an ID/Access token, or use the Admin Initiate Auth API for a test user. Call the protected endpoint with Authorization: Bearer \u0026lt;access_token\u0026gt;. Integrate from frontend Use @aws-amplify/auth or fetch to call Cognito login and attach the token to requests. Testing \u0026amp; troubleshooting\nVerify token audience (aud) matches the App Client ID. Check CORS settings on API Gateway for browser calls. Inspect CloudWatch Logs for Lambda and API Gateway execution errors. Cleanup\nDelete the Cognito User Pool, App Client, and API Gateway resources when finished. References\nAWS Cognito Console API Gateway (HTTP API) JWT authorizers "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Accelerate AI development with Amazon Bedrock API keys This blog introduces how to accelerate your AI application development by integrating and managing Amazon Bedrock API keys within your microservices architecture. You will learn why Bedrock API keys are a crucial tool for accessing and utilizing diverse AI models (such as Claude, Titan, Llama 2, and others) from multiple providers, and how microservices help make your system flexible, scalable, and easier to manage API access. The article will also guide you through the steps to set up the environment, organize the Bedrock API key access pipeline, and ensure compliance with security and IAM (Identity and Access Management) standards when working with AI.\nBlog 2 - New features and developer experience with enhanced Amazon Location Service This blog introduces how to leverage new features and the enhanced developer experience with Amazon Location Service. You will learn why these new updates are crucial for storing and analyzing diverse location data (such as maps, POIs, IoT device tracking data, etc.), how the new features help make your system flexible, scalable, and easier to integrate location services. The article will also guide you through the steps to set up the environment, organize the location data processing pipeline, and ensure compliance with security and privacy standards when using this service.\nBlog 3 - Accelerate VMware to AWS Migration with Trianz’s Rapid Migration Offer and Concierto platform This blog introduces how to accelerate the migration of VMware workloads to AWS by leveraging Trianz’s Rapid Migration Offer and the Concierto platform. You will learn why moving VMware to AWS helps organizations achieve higher scalability, cost optimization, and operational resilience. The article also explores how Trianz’s Concierto platform automates and simplifies the migration process—from discovery and assessment to deployment and optimization—ensuring minimal downtime and seamless transition. In addition, you will be guided through best practices for post-migration management, monitoring, and cost governance to maximize the benefits of AWS infrastructure.\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 - 16:40, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 - 16:30, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Workshop \u0026ldquo;Data science on AWS\u0026rdquo; – Unlock the Power of Data with Cloud Computing\nDate \u0026amp; Time: 9:30 - 11:45, October 16, 2025\nLocation: Hall A - FPTU HCM City\nRole: Attendee\nEvent 4 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 - 12:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 8:30 - 17:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 8:30 - 12:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.4-lambda-api-gateway/",
	"title": "Lambda + API Gateway",
	"tags": [],
	"description": "",
	"content": "Overview\nThis lab demonstrates how to build and deploy Lambda-backed HTTP APIs using Amazon API Gateway (HTTP API). Students will create a production-representative Lambda service (Node.js), integrate it with HTTP API routes, configure networking so Lambda can access an EC2-hosted SQL Server in a private subnet, secure credentials with Secrets Manager, and add observability (CloudWatch + X-Ray). The lab also covers IAM roles, CORS, payload format versions, cold-start considerations and deployment examples using AWS CLI / PowerShell.\nPrerequisites\nAWS account with permission to create Lambda, API Gateway, IAM roles, CloudWatch, VPC resources and Secrets Manager. AWS CLI v2 (or PowerShell configured) and optional SAM/Serverless framework for local testing. Node.js (LTS) for function development and bundling. Architecture notes\nFrontend (Vercel) calls API Gateway (HTTP API) endpoints. API Gateway routes requests to Lambda functions (Node.js). When Lambdas need to access SQL Server on EC2 (private subnet), configure Lambda to run in the same VPC subnets and allow network access via Security Groups and SSM if needed. Credentials (DB user/password) are stored in Secrets Manager; Lambda retrieves them at runtime (with IAM permission secretsmanager:GetSecretValue). Detailed steps\nPrepare Lambda execution role (IAM) Create a role with a trust policy for Lambda and attach the following minimal managed policies or inline policies: Example trust policy (lambda assume role):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Attach policies (examples):\nAWSLambdaBasicExecutionRole (CloudWatch Logs) Inline policy for Secrets Manager + ENI/VPC actions (example): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ {\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Action\u0026#34;:[\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\u0026#34;secretsmanager:DescribeSecret\u0026#34;],\u0026#34;Resource\u0026#34;:\u0026#34;arn:aws:secretsmanager:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:secret:\u0026lt;your-secret\u0026gt;\u0026#34;}, {\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Action\u0026#34;:[\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;,\u0026#34;ec2:DeleteNetworkInterface\u0026#34;],\u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;} ] } PowerShell / AWS CLI example to create role and attach policy (PowerShell):\n# create role $trust = \u0026#39;{\u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;,\u0026#34;Statement\u0026#34;:[{\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Principal\u0026#34;:{\u0026#34;Service\u0026#34;:\u0026#34;lambda.amazonaws.com\u0026#34;},\u0026#34;Action\u0026#34;:\u0026#34;sts:AssumeRole\u0026#34;}]}\u0026#39; aws iam create-role --role-name LearningHubLambdaRole --assume-role-policy-document $trust # attach managed policy aws iam attach-role-policy --role-name LearningHubLambdaRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # put inline policy for secrets + ENI # save the inline JSON to lambda-secrets-policy.json locally then run: aws iam put-role-policy --role-name LearningHubLambdaRole --policy-name LambdaSecretsPolicy --policy-document file://lambda-secrets-policy.json Build and package Lambda (Node.js) Project layout (example):\nlambda/ package.json index.js node_modules/\nUse a bundler (esbuild/webpack) or npm install --production and zip the folder.\nPowerShell example to package:\ncd lambda npm install --production Compress-Archive -Path * -DestinationPath ..\\lambda-package.zip Create Lambda function (CLI) aws lambda create-function --function-name learninghub-api-handler \\ --runtime nodejs18.x --handler index.handler \\ --zip-file fileb://lambda-package.zip \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/LearningHubLambdaRole \\ --timeout 30 --memory-size 512 If Lambda needs DB access in private subnet, update function configuration to include VPC config (subnet IDs + security group IDs):\naws lambda update-function-configuration --function-name learninghub-api-handler --vpc-config SubnetIds=subnet-aaa,subnet-bbb,SecurityGroupIds=sg-xxxx Notes on VPC:\nWhen Lambda runs in a VPC it creates ENIs in the subnets; this can increase cold-start time. Keep at least two ENI-capable subnets (private) available. If Lambda needs internet access (e.g., to call external APIs), ensure private subnets have a route to a NAT Gateway in a public subnet. Store DB credentials in Secrets Manager and grant Lambda access Create secret (PowerShell):\naws secretsmanager create-secret --name learninghub/sqlserver --secret-string \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;dbuser\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;P@ssw0rd\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;10.0.1.10\u0026#34;,\u0026#34;port\u0026#34;:\u0026#34;1433\u0026#34;}\u0026#39; Attach IAM permission (see step 1) so Lambda can GetSecretValue for that secret.\nCreate API Gateway (HTTP API) and integrate with Lambda Create HTTP API (CLI):\n$api=$(aws apigatewayv2 create-api --name learninghub-api --protocol-type HTTP --output json) $apiId=(ConvertFrom-Json $api).ApiId # create integration to Lambda $lambdaArn = \u0026#34;arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:function:learninghub-api-handler\u0026#34; $integration=$(aws apigatewayv2 create-integration --api-id $apiId --integration-type AWS_PROXY --integration-uri $lambdaArn --payload-format-version 2.0 --output json) $integrationId=(ConvertFrom-Json $integration).IntegrationId # create a route aws apigatewayv2 create-route --api-id $apiId --route-key \u0026#34;GET /status\u0026#34; --target \u0026#34;integrations/$integrationId\u0026#34; # add permission so API Gateway can invoke Lambda aws lambda add-permission --function-name learninghub-api-handler --statement-id apigw-invoke --action lambda:InvokeFunction --principal apigateway.amazonaws.com --source-arn arn:aws:execute-api:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:$apiId/*/* # deploy aws apigatewayv2 create-stage --api-id $apiId --stage-name prod --auto-deploy Notes:\nUse payload-format-version 2.0 for HTTP API; event shape differs from REST API. integration-type AWS_PROXY maps the entire HTTP request to Lambda event. Configure CORS (HTTP API) You can enable CORS in console or with CLI using update-api to set CorsConfiguration:\n$cors=\u0026#39;{\u0026#34;AllowOrigins\u0026#34;:[\u0026#34;https://learninghub.example.com\u0026#34;],\u0026#34;AllowMethods\u0026#34;:[\u0026#34;GET\u0026#34;,\u0026#34;POST\u0026#34;,\u0026#34;OPTIONS\u0026#34;],\u0026#34;AllowHeaders\u0026#34;:[\u0026#34;Content-Type\u0026#34;,\u0026#34;Authorization\u0026#34;],\u0026#34;MaxAge\u0026#34;:3600}\u0026#39; aws apigatewayv2 update-api --api-id $apiId --cors-configuration $cors Test the endpoint PowerShell example:\nInvoke-RestMethod -Method Get -Uri \u0026#34;https://$apiId.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/status\u0026#34; If your route is protected (e.g., by Cognito authorizer) attach token header: -Headers @{ Authorization = \u0026quot;Bearer $token\u0026quot; }.\nObservability \u0026amp; best practices CloudWatch Logs: set log retention, enable structured logging (JSON) from Lambda. X-Ray: enable active tracing on the Lambda function for distributed tracing. Metrics \u0026amp; Alarms: create alarms for Errors, Throttles, Duration. API access logs: configure stage-level access logging for HTTP API via accessLogSettings in create-stage/update-stage. Security Use Secrets Manager for DB credentials; avoid environment variables with plaintext secrets. Scope IAM policies to least privilege and specific ARNs when possible. When running Lambda in VPC, ensure security group rules allow outbound to the DB (port 1433) and inbound rules on the DB EC2 allow only Lambda SGs. Cleanup # delete stage and API aws apigatewayv2 delete-api --api-id $apiId # delete lambda aws lambda delete-function --function-name learninghub-api-handler # delete role inline policy, detach managed, then delete role aws iam delete-role-policy --role-name LearningHubLambdaRole --policy-name LambdaSecretsPolicy aws iam detach-role-policy --role-name LearningHubLambdaRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name LearningHubLambdaRole Appendix: sample minimal Lambda (Node.js)\n// index.js const sql = require(\u0026#39;mssql\u0026#39;); // if using mssql package exports.handler = async (event) =\u0026gt; { // parse request, read secret, connect to SQL Server, run query return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ message: \u0026#39;OK\u0026#39;, request: event }) }; }; References\nAWS Lambda Developer Guide Amazon API Gateway (HTTP API) Developer Guide AWS Secrets Manager Developer Guide "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.5-ec2-privatesubnet/",
	"title": "EC2 (Private Subnet) — Windows + SQL Server",
	"tags": [],
	"description": "",
	"content": "Overview\nThis lab covers deploying and operating a Windows EC2 instance running SQL Server Express in a private subnet designed for internal-only database access. The document focuses on network design (VPC, route tables, NAT, VPC endpoints), secure administration via AWS Systems Manager (SSM), SQL Server silent installation and hardening, backup/restore (EBS snapshots and SQL backups to S3), monitoring, and production-ready operational practices.\nPrerequisites\nAWS account with permissions to create VPCs, subnets, route tables, NAT Gateways, EC2, IAM roles, Systems Manager, EBS, and S3. AWS CLI v2 or Console access and basic familiarity with EC2, VPC and IAM concepts. An existing VPC or permission to create one for the lab. Learning objectives\nDesign a VPC with public and private subnets and correct routing for private DB instances. Launch a Windows EC2 instance in private subnets and configure it securely for SQL Server. Use AWS Systems Manager (Session Manager / Run Command / Patch Manager) for administration without exposing RDP. Implement backups (EBS snapshots and SQL backups), monitoring, and basic hardening. Network \u0026amp; VPC design\nRecommended subnet layout:\nPublic subnets: NAT Gateway, load balancers, bastion (if used). Private subnets: database instances, application services (Lambda or ECS tasks in private subnets). At least two AZs with private subnets for resilience. Route tables:\nPublic route table: route 0.0.0.0/0 -\u0026gt; Internet Gateway (IGW). Private route table: route 0.0.0.0/0 -\u0026gt; NAT Gateway (for OS updates/outbound), or no NAT if using SSM/VPC endpoints for patching. VPC Endpoints:\nAdd Interface endpoints for com.amazonaws.\u0026lt;region\u0026gt;.ssm, com.amazonaws.\u0026lt;region\u0026gt;.ec2messages, com.amazonaws.\u0026lt;region\u0026gt;.ssmmessages to allow SSM traffic without Internet. Consider Gateway endpoint for S3 if you push backups to S3 (reduces public egress). Security groups \u0026amp; NACLs\nSecurity Group for SQL Server (db-sg):\nInbound: TCP 1433 from application subnets/SGs only (use Security Group IDs for source when possible). Outbound: restrict as needed; at minimum allow response traffic. Management Security Group (mgmt-sg):\nAllow SSM related traffic (SSM uses agent outbound to AWS endpoints; with VPC endpoints you don\u0026rsquo;t need IGW/NAT for SSM). Network ACLs: keep default permissive rules or implement stateless filters if required by policy; Security Groups are primary control.\nIAM \u0026amp; instance profile\nCreate an IAM role for EC2 with the following managed policy to enable SSM:\nAmazonSSMManagedInstanceCore Additional policies (add least privilege scope):\nPermission to create \u0026amp; describe EBS snapshots if instance will trigger snapshots: ec2:CreateSnapshot, ec2:DescribeVolumes, etc. Permission to upload backups to S3 if using instance-based uploads (consider using a separate S3 role/policy limited to a prefix). Instance type, storage, and AMI choices\nInstance type: choose based on SQL Server workloads (t3.medium or t3.large for light dev/test; r5/ m5 for heavier loads). For production, consider dedicated RDS or EC2 with provisioned IOPS. Storage: Separate volumes: OS (C:) and Data (E: or D:) volumes. Put SQL Data, Logs, TempDB on separate EBS volumes for performance and snapshot granularity. Use gp3 or io2 for consistent IOPS depending on workload. Enable EBS encryption (KMS) for sensitive data. Windows \u0026amp; SQL Server installation (automation)\nInstall via SSM Run Command or using a pre-baked AMI with SQL Server/agents installed. Example PowerShell sequence (SSM Run Command or Session Manager): PowerShell (on instance) - silent install example for SQL Server Express:\n# Download installer $url = \u0026#34;https://download.microsoft.com/.../SQLEXPR_x64_ENU.exe\u0026#34; Invoke-WebRequest -Uri $url -OutFile C:\\Temp\\sqlexpr.exe # Create configuration file (simple example) $config = @\u0026#34; [OPTIONS] ACTION=Install FEATURES=SQLENGINE INSTANCENAME=SQLEXPRESS SECURITYMODE=SQL SAPWD=\u0026#34;P@ssw0rd\u0026#34; SQLSVCACCOUNT=\u0026#34;NT AUTHORITY\\SYSTEM\u0026#34; TCPENABLED=1 NPENABLED=0 \u0026#34;@ $config | Out-File C:\\Temp\\ConfigurationFile.ini -Encoding ascii # Run silent installer \u0026amp; C:\\Temp\\sqlexpr.exe /Q /ACTION=Install /IACCEPTSQLSERVERLICENSETERMS /ConfigurationFile=C:\\Temp\\ConfigurationFile.ini After install, enable TCP/IP protocol in SQL Server Configuration Manager (can be scripted via PowerShell/registry) and restart SQL Server service. Configure SQL Server Mixed Mode if you need SQL authentication; create a dedicated SQL login for app access. Firewall \u0026amp; Windows Defender\nConfigure Windows Firewall to allow inbound TCP 1433 only from trusted source ranges or SGs (if using security group references via AWS-managed firewall integration, still set host firewall). Administration via SSM\nUse Session Manager for interactive shell access and port forwarding to avoid exposing RDP. Example to start a session: aws ssm start-session --target i-0123456789abcdef0 Port forwarding example (local RDP tunneled over Session Manager): Start-SSMSession -Target i-0123456789abcdef0 -DocumentName AWS-StartPortForwardingSession -Parameters @{\u0026#34;portNumber\u0026#34;=[\u0026#34;3389\u0026#34;];\u0026#34;localPortNumber\u0026#34;=[\u0026#34;13389\u0026#34;]} Connectivity from Lambda or app\nIf using Lambda in the same VPC, ensure Lambda\u0026rsquo;s vpcConfig includes private subnet IDs and security group IDs that allow outbound to db-sg on 1433. Remember Lambda will create ENIs in the subnets which affects cold-start.\nTesting connectivity:\nFrom a bastion or EC2 in same subnet: Test-NetConnection -ComputerName 10.0.x.10 -Port 1433 (PowerShell) Or use tcping/telnet for TCP checks. Backups and restore\nTwo layers of backups recommended:\nSQL-native backups: full/diff/log backups scheduled and stored to disk, then copied to S3 (use SSM or custom backup agent/script). Infrastructure backups: EBS snapshots for volume-level recovery (fast restore). Automate snapshots with AWS Data Lifecycle Manager (DLM) or Lambda triggered by CloudWatch Events.\nRestore test: create volume from snapshot, attach to a recovery instance, mount and verify SQL files or restore .bak files to a new SQL Server instance.\nMonitoring, logging, and patching\nInstall and configure CloudWatch agent to collect Windows performance counters and custom logs. Enable CloudWatch Logs for SSM and EC2; set log retention and metric filters for alerts. Use SSM Patch Manager to apply OS updates on a maintenance window without opening internet-facing management ports. Hardening \u0026amp; security\nPrinciple of least privilege for IAM roles and S3 prefixes. Use Security Group references (SG ID) rather than CIDR when restricting DB access. Enable EBS encryption and consider using a customer-managed KMS key when required. Disable unnecessary Windows features and remove local admin accounts where possible; use AWS IAM Identity Center or AD integration for enterprise setups. Troubleshooting checklist\nSSM connectivity: check SSM agent, instance role, VPC endpoints for SSM, and CloudWatch logs for agent errors. Network: verify route tables, NACLs, SG rules, and source/destination checks for NAT instances. SQL: check SQL Server error logs (in C:\\Program Files\\Microsoft SQL Server\\MSSQL..\\MSSQL\\Log), verify TCP/IP enabled and SQL service running. Cleanup\nTerminate EC2 instance, delete EBS snapshots and any temporary S3 objects, remove IAM role/policies and DLM lifecycle policies created for snapshots. References \u0026amp; further reading\nAWS Systems Manager Session Manager Amazon VPC design and NAT Gateway documentation SQL Server Express silent install and configuration guidance AWS Data Lifecycle Manager (DLM) for EBS snapshots "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Workshop — LearningHub (2HTD) Overview This workshop accompanies the 2HTD-LearningHub proposal and provides step-by-step labs to deploy and validate the project\u0026rsquo;s main components: frontend hosted on Vercel, serverless backend with AWS Lambda + API Gateway, relational data stored in SQL Server on EC2 (private subnet), and media stored in Amazon S3. The labs focus on secure hybrid access patterns, VPC design, EC2 administration via AWS Systems Manager (SSM), presigned S3 uploads, Lambda-based APIs, and Cognito authentication.\nLabs are short and focused; they can run in a compact test environment (single VPC) or in an expanded setup (cloud VPC + on-prem simulation) to demonstrate different network flows and NAT/egress trade-offs.\nLearning objectives Understand the LearningHub hybrid architecture (Vercel + Lambda + EC2 + S3). Provision VPCs and subnets, and design NAT / VPC endpoints to control egress costs. Deploy EC2 Windows + SQL Server Express and manage it securely using AWS Systems Manager (SSM). Configure S3 buckets and implement uploads using presigned URLs from the frontend. Deploy Lambda (Node.js) functions and expose them via API Gateway (HTTP API). Configure Amazon Cognito for authentication and test protected API flows. Project Architecture The LearningHub system uses a frontend-hosted + serverless backend pattern with a Microsoft SQL Server running on an EC2 instance inside a private subnet for relational data. Key components:\nFrontend: Static or SSR Next.js app hosted on Vercel, serving the UI and calling APIs. API: Amazon API Gateway (HTTP API) forwarding requests to AWS Lambda (Node.js) for business logic. Storage: Amazon S3 for media storage; uploads use presigned URLs from the frontend. Database: Microsoft SQL Server on Amazon EC2 in a private subnet (accessed from Lambda via private networking or via approved routes). Authentication: Amazon Cognito issues tokens for frontend authentication and protects API endpoints. Network \u0026amp; Security: VPC, subnets, security groups, IAM roles, and VPC Endpoints for S3 to reduce egress; NAT Gateway for egress where required. Ops \u0026amp; Observability: AWS Systems Manager (SSM) for EC2 administration, CloudWatch for logs/metrics, and AWS Secrets Manager / Parameter Store for secrets. Main Request / Data Flows The user loads the frontend from Vercel; if authentication is required the frontend redirects to Cognito and receives an access token. For media upload, the frontend requests a presigned URL from an API endpoint (API Gateway → Lambda), passing the user\u0026rsquo;s token where required. The Lambda function generates a presigned S3 URL using the SDK and returns it; the frontend uploads directly to S3 with the presigned URL. After upload, the frontend may call a confirmation API (API Gateway → Lambda) to record metadata or trigger downstream processing. Business requests (CRUD operations, user data) go through API Gateway → Lambda; Lambdas may query or update the SQL Server on EC2 over the private network or use S3 as an intermediary for large objects. EC2 management tasks (SQL setup, patching) are performed via AWS Systems Manager (SSM Session Manager) without opening RDP/SSH to the internet. To minimize egress costs when accessing S3 from inside the VPC, enable an S3 VPC Endpoint; for external package downloads or other internet egress, consider a NAT Gateway (cost trade-off) or use SSM + endpoints. These flows map directly to the workshop labs: presigned S3 upload, S3 access from VPC (with endpoints), EC2 management via SSM, and Lambda APIs connecting to EC2/DB.\nAWS Services Used Category Service Compute AWS Lambda (Function / Container), Amazon EC2 (Windows + SQL Server Express) Storage Amazon S3 API Amazon API Gateway (HTTP API) Authentication \u0026amp; Security Amazon Cognito, AWS IAM, AWS Secrets Manager Monitoring Amazon CloudWatch Logs \u0026amp; Metrics Network VPC, Subnets, NAT Gateway, Internet Gateway, VPC Endpoints DNS Amazon Route 53 IaC / CI-CD Terraform (or CDK), GitHub Actions Ops \u0026amp; Backup AWS Systems Manager (SSM), EBS Snapshots Time \u0026amp; Cost Estimate Item Detail Time 3–4 hours (short lab) Level Intermediate Estimated Cost Workshop: minimal (temporary lab resources); Full deployment (production): ≈ 60.00 USD / month (see Proposal) Note: Workshop costs depend on runtime hours and resource choices. The full-system monthly estimate is available in the Proposal section.\nLab index Workshop overview Prerequisite \u0026amp; environment setup Integration Cognito + API Gateway Lambda + API Gateway (Serverless API) EC2 Windows + SQL Server (Private Subnet) Cleanup "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the 2HTD_LearningHub project on the web for learning and testing English., through which I improved my skills in programming, analytical thinking, clear report writing, and effective communication in both study and teamwork.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/1-worklog/1.2-week2/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "title: \u0026ldquo;Week 2 Worklog\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 1.2. \u0026quot;\nWeek 2 — Objectives Get hands-on with S3 static website hosting, RDS (MySQL) and Route53 basics. Learn how to connect an EC2 client to an RDS instance and configure IAM/Security Groups for DB access. Planned tasks this week Day Task Start Date Completion Date Reference Material 2 - Create an S3 bucket to host a static website\n- Upload demo HTML/CSS files to the bucket 15/09/2025 15/09/2025 AWS Journey 3 - Enable Static Website Hosting on the S3 bucket\n- Configure bucket policy to allow public read for site assets\n- Access the site via the S3 website endpoint 16/09/2025 16/09/2025 AWS Journey 4 - Launch an RDS MySQL instance (Free Tier for lab)\n- Configure VPC Security Group to allow connections from application subnets/EC2\n- Record endpoint and credentials 17/09/2025 17/09/2025 AWS Journey 5 - Launch an EC2 instance and install MySQL client/tools\n- From EC2, connect to the RDS endpoint using CLI/mysql client and create a test database/table 18/09/2025 18/09/2025 AWS Journey 6 - Learn Route53 basics and create a Hosted Zone\n- Create A / CNAME records to point a domain to the S3 static site\n- Verify domain access to the website 19/09/2025 19/09/2025 AWS Journey Week 2 — Outcomes Built and hosted a static website in S3 and verified public access via the S3 endpoint. Configured Route53 to map a domain to the S3 site (A/CNAME) and validated domain access. Created an RDS MySQL instance and connected to it from an EC2 client; recorded endpoint and credentials for testing. Learned to configure Security Groups and minimal IAM roles relevant to RDS and S3. Recommended next topics for Week 3: CloudFront, DynamoDB, and ElastiCache for caching and scale testing. "
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://pmtuan171204.github.io/tuanpmse180595/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]